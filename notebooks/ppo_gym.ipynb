{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "d7a86d02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Общее конфигурирование\n",
        "from agentslab.utils.device import resolve_device\n",
        "from agentslab.utils.seeding import set_global_seed\n",
        "from pathlib import Path\n",
        "\n",
        "device = resolve_device(\"cpu\")\n",
        "print('Device:', device)\n",
        " \n",
        "seed = 42\n",
        "set_global_seed(seed, deterministic=True)\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "ENV_ID = \"InvertedDoublePendulum-v4\"\n",
        "# ENV_ID = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "8a620aba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log dir: C:\\Users\\ordevoir\\Documents\\GitHub\\AgentsLab\\logs\\ppo_gym_demo_20250822-040316\n",
            "Checkpoint dir: C:\\Users\\ordevoir\\Documents\\GitHub\\AgentsLab\\checkpoints\\ppo_InvertedDoublePendulum-v4_demo_20250822-040316\n"
          ]
        }
      ],
      "source": [
        "# DRAFT\n",
        "\n",
        "from datetime import datetime\n",
        "from agentslab.utils.checkpointers import CheckpointInfo, save_checkpoint, load_checkpoint\n",
        "\n",
        "run_name = f\"ppo_gym_demo_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "\n",
        "LOGS_ROOT = (ROOT / 'logs').resolve()\n",
        "CKPT_ROOT = (ROOT / 'checkpoints').resolve()\n",
        "LOGS_ROOT.mkdir(exist_ok=True)\n",
        "CKPT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "log_dir = LOGS_ROOT / run_name\n",
        "ckpt_info = CheckpointInfo(algo='ppo', env_id='InvertedDoublePendulum-v4', run_name='demo', dir_root=str(CKPT_ROOT))\n",
        "run_ckpt_dir = ckpt_info.make_run_dir()\n",
        "\n",
        "from agentslab.utils.logger import CSVLogger\n",
        "from agentslab.utils.curves import plot_training_curves\n",
        "\n",
        "\n",
        "logger = CSVLogger(str(log_dir))\n",
        "\n",
        "print('Log dir:', log_dir)\n",
        "print('Checkpoint dir:', run_ckpt_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54dd996",
      "metadata": {},
      "source": [
        "# Создание среды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "6d131b0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m2025-08-22 04:03:18,361 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from agentslab.envs.gym_factory import GymEnvConfig, make_gym_env\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "env_cfg = GymEnvConfig(env_id=ENV_ID, render_mode=None, device=device, seed=seed)\n",
        "env = make_gym_env(env_cfg)\n",
        "check_env_specs(env)\n",
        "\n",
        "# from agentslab.utils.specs import print_specs\n",
        "# print_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c427be",
      "metadata": {},
      "source": [
        "# Создание актора и критика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "d3824baf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ValueOperator(\n",
              "    module=Sequential(\n",
              "      (0): Linear(in_features=11, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    ),\n",
              "    device=cpu,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['state_value'])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.modules.networks import MLPConfig, build_mlp\n",
        "from agentslab.modules.policy import build_stochastic_actor\n",
        "from torchrl.modules import ValueOperator\n",
        "\n",
        "# Достаём размерности\n",
        "obs_dim = env.observation_spec[\"observation\"].shape[-1]\n",
        "act_dim = env.action_spec.shape[-1]\n",
        "\n",
        "mlp_cfg = MLPConfig(\n",
        "        in_dim = obs_dim, \n",
        "        out_dim = 2*act_dim,\n",
        "        hidden_sizes = (256, 256),\n",
        "        activation = \"tanh\",\n",
        "        layer_norm = False\n",
        ")\n",
        "\n",
        "actor_network = build_mlp(mlp_cfg)\n",
        "actor = build_stochastic_actor(actor_network, env.action_spec)\n",
        "\n",
        "mlp_cfg.out_dim = act_dim\n",
        "critic_network = build_mlp(mlp_cfg)\n",
        "critic = ValueOperator(module=critic_network, in_keys=[\"observation\"])\n",
        "critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebea77ec",
      "metadata": {},
      "source": [
        "# Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "6a9cdc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "frames_per_batch = 1000\n",
        "# For a complete training, bring the number of frames up to 1M\n",
        "total_frames = 10_000\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    create_env_fn=env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    split_trajs=False,\n",
        "    device=device,\n",
        ")\n",
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8859fad",
      "metadata": {},
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "e45ce2a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "import torch\n",
        "\n",
        "gamma = 0.99\n",
        "lmbda = 0.95\n",
        "\n",
        "advantage_module = GAE(\n",
        "    gamma=gamma, lmbda=lmbda, value_network=critic, average_gae=True\n",
        ")\n",
        "\n",
        "clip_epsilon = (\n",
        "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
        ")\n",
        "entropy_eps = 1e-4\n",
        "\n",
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_bonus=bool(entropy_eps),\n",
        "    entropy_coeff=entropy_eps,\n",
        "    # these keys match by default but we set this for completeness\n",
        "    critic_coeff=1.0,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        ")\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optim, \n",
        "    T_max=total_frames // frames_per_batch, \n",
        "    eta_min=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfa17c6",
      "metadata": {},
      "source": [
        "\n",
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "399b9375",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_logfile(log_dir=\"logs\", name=None, run_name=None, fieldnames=None):\n",
        "    import os, csv, time\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    run_name = run_name or f\"run_{ts}\"\n",
        "    if name:\n",
        "        run_name = name + \"_\" + run_name\n",
        "    log_path = os.path.join(log_dir, f\"{run_name}.csv\")\n",
        "    fieldnames = fieldnames or [\"step\",\"metric\",\"value\"]  # дефолт\n",
        "    with open(log_path, \"w\", newline=\"\") as f:\n",
        "        csv.DictWriter(f, fieldnames=fieldnames).writeheader()\n",
        "    return log_path\n",
        "\n",
        "def log_row(log_path, fieldnames, row: dict):\n",
        "    import csv\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        csv.DictWriter(f, fieldnames=fieldnames).writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "3967ac3e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'logs\\\\ppo_run_20250822-040318.csv'"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fieldnames = [\n",
        "    \"batch_idx\",\n",
        "    \"frames\",\n",
        "    \"reward_mean\",\n",
        "    \"reward_init\",\n",
        "    \"step_count_max\",\n",
        "    \"lr\",\n",
        "    \"eval_reward_mean\",\n",
        "    \"eval_reward_sum\",\n",
        "    \"eval_step_count\",\n",
        "]\n",
        "\n",
        "log_path = prepare_logfile(fieldnames=fieldnames, name=\"ppo\")\n",
        "log_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728107a0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f4af448a",
      "metadata": {},
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "7abe3f58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "# import torch\n",
        "\n",
        "# from typing import Dict\n",
        "# @torch.no_grad()\n",
        "# def eval_policy(env, actor, n_steps: int = 1000) -> Dict[str, float]:\n",
        "#     \"\"\"\n",
        "#     Выполняет детерминированный rollout политики и возвращает метрики.\n",
        "#     \"\"\"\n",
        "#     # На время оценки выключаем стохастику действий\n",
        "#     actor_was_training = actor.training\n",
        "#     actor.eval()\n",
        "#     with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "#         eval_rollout = env.rollout(n_steps, actor)\n",
        "#         result = {\n",
        "#             \"eval_reward_mean\": eval_rollout[\"next\", \"reward\"].mean().item(),\n",
        "#             \"eval_reward_sum\": eval_rollout[\"next\", \"reward\"].sum().item(),\n",
        "#             \"eval_step_count\": eval_rollout[\"step_count\"].max().item(),\n",
        "#         }\n",
        "#         del eval_rollout\n",
        "#     if actor_was_training:\n",
        "#         actor.train()\n",
        "#     return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192792e9",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "785ade9f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d94acaf329724dce99f3f052803ae804",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "eval:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'return_mean': 69.00655708312988,\n",
              " 'return_sum': 3450.327854156494,\n",
              " 'max_episode_lengh': 10,\n",
              " 'num_episodes': 50}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "eval_policy(env, actor, episodes=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dd8e2e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c046efe",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "TensorboardLogger.__init__() got an unexpected keyword argument 'flush_secs'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m csv_train = CSVLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     11\u001b[39m csv_eval  = CSVLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tb_train  = \u001b[43mTensorboardLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXP_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOG_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m tb_eval   = TensorboardLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \u001b[33m\"\u001b[39m\u001b[33mtb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m), flush_secs=\u001b[32m1\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_all\u001b[39m(loggers, name, value, step):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Унифицированный вызов на все бэкенды\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: TensorboardLogger.__init__() got an unexpected keyword argument 'flush_secs'"
          ]
        }
      ],
      "source": [
        "# === логгирование ===\n",
        "import os, time\n",
        "from collections import defaultdict\n",
        "from torchrl.record import CSVLogger, TensorboardLogger  # единое API логгеров TorchRL\n",
        "\n",
        "EXP_NAME = \"ppo_marl\"\n",
        "LOG_ROOT = \"../logs\"\n",
        "\n",
        "# Раздельные логгеры под train и eval, и сразу в два бэкенда\n",
        "csv_train = CSVLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \"csv\", \"train\"))\n",
        "csv_eval  = CSVLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \"csv\", \"eval\"))\n",
        "tb_train  = TensorboardLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \"tb\", \"train\"))\n",
        "tb_eval   = TensorboardLogger(exp_name=EXP_NAME, log_dir=os.path.join(LOG_ROOT, \"tb\", \"eval\"))\n",
        "\n",
        "def _log_all(loggers, name, value, step):\n",
        "    # Унифицированный вызов на все бэкенды\n",
        "    for lg in loggers:\n",
        "        lg.log_scalar(name, float(value), step=step)\n",
        "\n",
        "# (необязательно) сохраним гиперпараметры один раз\n",
        "hparams = {\n",
        "    \"algo\": \"PPO\",\n",
        "    \"total_frames\": int(total_frames),\n",
        "    \"num_epochs\": int(num_epochs),\n",
        "    \"sub_batch_size\": int(sub_batch_size),\n",
        "    \"max_grad_norm\": float(1.0),\n",
        "    \"frames_per_batch\": int(frames_per_batch),\n",
        "    \"lr\": float(optim.param_groups[0][\"lr\"]),\n",
        "}\n",
        "for lg in (csv_train, tb_train, csv_eval, tb_eval):\n",
        "    lg.log_hparams(hparams)\n",
        "\n",
        "# === метрики в памяти (по желанию) ===\n",
        "from tqdm.auto import tqdm\n",
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "logs = defaultdict(list)\n",
        "losses = []\n",
        "\n",
        "pbar = tqdm(total=total_frames, dynamic_ncols=True, leave=True)\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_epochs = 10\n",
        "sub_batch_size = 64\n",
        "\n",
        "global_frames = 0\n",
        "t0 = time.time()\n",
        "\n",
        "# Итерируемся по коллекторам, пока не наберём нужное число шагов\n",
        "for i, tensordict_data in enumerate(collector):\n",
        "\n",
        "    # ---- оптимизация на партии ----\n",
        "    # будем усреднять лоссы и норму градиента по всем sub-итерациям\n",
        "    loss_sum = defaultdict(float)\n",
        "    grad_norm_sum = 0.0\n",
        "    opt_steps = 0\n",
        "\n",
        "    for _ in range(num_epochs):\n",
        "        advantage_module(tensordict_data)\n",
        "        data_view = tensordict_data.reshape(-1)\n",
        "        replay_buffer.extend(data_view.cpu())\n",
        "\n",
        "        for _ in range(frames_per_batch // sub_batch_size):\n",
        "            subdata = replay_buffer.sample(sub_batch_size)\n",
        "            loss_vals = loss_module(subdata.to(device))\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"]\n",
        "                + loss_vals[\"loss_critic\"]\n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "            loss_value.backward()\n",
        "            # clip_grad_norm_ возвращает общую норму до клипа\n",
        "            total_norm = torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            # аккумулируем для усреднения\n",
        "            loss_sum[\"objective\"] += float(loss_vals[\"loss_objective\"].item())\n",
        "            loss_sum[\"critic\"]    += float(loss_vals[\"loss_critic\"].item())\n",
        "            loss_sum[\"entropy\"]   += float(loss_vals[\"loss_entropy\"].item())\n",
        "            grad_norm_sum         += float(total_norm.item())\n",
        "            opt_steps += 1\n",
        "\n",
        "    scheduler.step()  # Шаг LR-планировщика\n",
        "\n",
        "    # ---- сбор «сырых» метрик партии ----\n",
        "    reward_mean = tensordict_data[\"next\", \"reward\"].mean().item()\n",
        "    step_count_max = tensordict_data[\"step_count\"].max().item()\n",
        "    lr_now = optim.param_groups[0][\"lr\"]\n",
        "\n",
        "    logs[\"reward\"].append(reward_mean)\n",
        "    logs[\"step_count\"].append(step_count_max)\n",
        "    logs[\"lr\"].append(lr_now)\n",
        "\n",
        "    # корректное число фреймов в пачке и обновление глобального счётчика\n",
        "    batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "        global_frames += inc\n",
        "\n",
        "    # ---- логгируем train-метрики (общий step = собранные кадры) ----\n",
        "    train_loggers = (csv_train, tb_train)\n",
        "    # усреднённые лоссы за текущую большую партию\n",
        "    if opt_steps > 0:\n",
        "        _log_all(train_loggers, \"train/loss_objective\",  loss_sum[\"objective\"] / opt_steps, step=global_frames)\n",
        "        _log_all(train_loggers, \"train/loss_critic\",     loss_sum[\"critic\"]    / opt_steps, step=global_frames)\n",
        "        _log_all(train_loggers, \"train/loss_entropy\",    loss_sum[\"entropy\"]   / opt_steps, step=global_frames)\n",
        "        _log_all(train_loggers, \"train/grad_norm_mean\",  grad_norm_sum / opt_steps,         step=global_frames)\n",
        "\n",
        "    # базовые скаляры\n",
        "    _log_all(train_loggers, \"train/reward_mean\", reward_mean, step=global_frames)\n",
        "    _log_all(train_loggers, \"train/step_count_max\", step_count_max, step=global_frames)\n",
        "    _log_all(train_loggers, \"train/lr\", lr_now, step=global_frames)\n",
        "    _log_all(train_loggers, \"train/fps\", global_frames / max(1e-6, (time.time() - t0)), step=global_frames)\n",
        "\n",
        "    # ---- периодическая оценка политики (eval) ----\n",
        "    if (i + 1) % 5 == 0:\n",
        "        eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "\n",
        "        # Логируем любые числовые ключи из eval_results с префиксом eval/\n",
        "        eval_loggers = (csv_eval, tb_eval)\n",
        "        for k, v in eval_results.items():\n",
        "            if isinstance(v, (int, float)):\n",
        "                _log_all(eval_loggers, f\"eval/{k}\", v, step=global_frames)\n",
        "\n",
        "        # печать рядом с прогресс-баром\n",
        "        try:\n",
        "            pbar.write(\n",
        "                f'eval: avg reward = {eval_results[\"return_mean\"]}, '\n",
        "                f'max episode length = {eval_results.get(\"max_episode_length\", eval_results.get(\"max_episode_lengh\", \"NA\"))}'\n",
        "            )\n",
        "        except Exception:\n",
        "            pbar.write(f\"eval: {eval_results}\")\n",
        "\n",
        "    # ---- описание прогресс-бара ----\n",
        "    avg_reward_str = f\"avg reward={reward_mean: 4.4f}\"\n",
        "    step_count_str = f\"max step count: {step_count_max}\"\n",
        "    lr_str = f\"lr: {lr_now: 4.4f}\"\n",
        "    pbar.set_description(\", \".join([avg_reward_str, step_count_str, lr_str]))\n",
        "\n",
        "# Гарантированно закрываем бар\n",
        "pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb7e794",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'фывафыв' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mфывафыв\u001b[49m\n",
            "\u001b[31mNameError\u001b[39m: name 'фывафыв' is not defined"
          ]
        }
      ],
      "source": [
        "фывафыв"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770b537f",
      "metadata": {},
      "outputs": [],
      "source": [
        "!tensorboard --logdir ./logs/tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5900d0ef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20ac31b169d24bae8794210950d87225",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "from agentslab.runners.evals import eval_policy\n",
        "from collections import defaultdict\n",
        "\n",
        "logs = defaultdict(list)\n",
        "losses = []\n",
        "\n",
        "pbar = tqdm(total=total_frames, dynamic_ncols=True, leave=True)\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_epochs = 10\n",
        "sub_batch_size = 64\n",
        "\n",
        "# Итерируемся по коллекторам, пока не наберём нужное число шагов\n",
        "for i, tensordict_data in enumerate(collector):\n",
        "    # Учимся на партии данных\n",
        "    for _ in range(num_epochs):\n",
        "        # Advantage для PPO пересчитываем на каждом проходе\n",
        "        advantage_module(tensordict_data)\n",
        "        data_view = tensordict_data.reshape(-1)\n",
        "        replay_buffer.extend(data_view.cpu())\n",
        "        for _ in range(frames_per_batch // sub_batch_size):\n",
        "            subdata = replay_buffer.sample(sub_batch_size)\n",
        "            loss_vals = loss_module(subdata.to(device))\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"] \n",
        "                + loss_vals[\"loss_critic\"] \n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "            # Оптимизация\n",
        "            loss_value.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "    scheduler.step()            # Шаг LR-планировщика\n",
        "\n",
        "    # Логируем метрики\n",
        "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
        "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
        "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "    # Оценка после каждых 5 партий (соответствует условию ниже)\n",
        "    if (i + 1) % 5 == 0:\n",
        "        eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "        # pbar.write не ломает отрисовку прогресс-бара\n",
        "        pbar.write(f\"eval: avg reward = {eval_results[\"return_mean\"]}, max episode lengh = {eval_results[\"max_episode_lengh\"]}\")\n",
        "\n",
        "    # Обновляем описание прогресс-бара\n",
        "    avg_reward_str = f\"avg reward={logs['reward'][-1]: 4.4f}\"\n",
        "    step_count_str = f\"max step count: {logs['step_count'][-1]}\"\n",
        "    lr_str = f\"lr: {logs['lr'][-1]: 4.4f}\"\n",
        "    \n",
        "    # корректное число фреймов в пачке\n",
        "    batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "    # аккуратное обновление бара\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "\n",
        "    pbar.set_description(\", \".join([avg_reward_str, step_count_str, lr_str]))\n",
        "\n",
        "# Гарантированно закрываем бар (важно для корректного вывода в ноутбуках)\n",
        "pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4447b00c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(logs[\"reward\"])\n",
        "plt.title(\"training rewards (average)\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(logs[\"step_count\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60add307",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'EvalConfig' from 'agentslab.runners.evals' (C:\\Users\\ordevoir\\Documents\\GitHub\\AgentsLab\\src\\agentslab\\runners\\evals.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 3: Конфигурирование trainer и запуск обучения\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentslab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPOTrainer, PPOTrainerConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentslab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvalConfig, evaluate_policy\n\u001b[32m      5\u001b[39m trainer_cfg = PPOTrainerConfig(\n\u001b[32m      6\u001b[39m     frames_per_batch=\u001b[32m1024\u001b[39m,\n\u001b[32m      7\u001b[39m     total_frames=\u001b[32m20_480\u001b[39m,  \u001b[38;5;66;03m# демо; поднимайте до 1e6 для реальных результатов\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     eval_every=\u001b[32m5\u001b[39m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluator\u001b[39m():\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'EvalConfig' from 'agentslab.runners.evals' (C:\\Users\\ordevoir\\Documents\\GitHub\\AgentsLab\\src\\agentslab\\runners\\evals.py)"
          ]
        }
      ],
      "source": [
        "# Cell 3: Конфигурирование trainer и запуск обучения\n",
        "from agentslab.runners.trainers import PPOTrainer, PPOTrainerConfig\n",
        "from agentslab.runners.evals import EvalConfig, evaluate_policy\n",
        "\n",
        "trainer_cfg = PPOTrainerConfig(\n",
        "    frames_per_batch=1024,\n",
        "    total_frames=20_480,  # демо; поднимайте до 1e6 для реальных результатов\n",
        "    sub_batch_size=64,\n",
        "    num_epochs=10,\n",
        "    lr=3e-4,\n",
        "    max_grad_norm=1.0,\n",
        "    gamma=0.99,\n",
        "    lam=0.95,\n",
        "    clip_epsilon=0.2,\n",
        "    entropy_coef=1e-4,\n",
        "    seed=seed,\n",
        "    device=device,\n",
        "    eval_every=5,\n",
        ")\n",
        "\n",
        "def evaluator():\n",
        "    return evaluate_policy(env, actor, EvalConfig(steps=1000, device=device))\n",
        "\n",
        "trainer = PPOTrainer(trainer_cfg, env, actor, critic, run_dirs={'ckpt': run_ckpt_dir, 'logs': str(log_dir)}, logger=logger)\n",
        "trainer.train(evaluator=evaluator)\n",
        "\n",
        "# Сохранение чекпоинта\n",
        "ckpt_path = save_checkpoint(run_ckpt_dir, actor, critic, None, None, extra={'iter_done': trainer_cfg.total_frames // trainer_cfg.frames_per_batch})\n",
        "print('Saved to:', ckpt_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Конфигурирование и оценка модели\n",
        "from agentslab.runners.evals import EvalConfig, evaluate_policy\n",
        "eval_res = evaluate_policy(env, actor, EvalConfig(steps=1000, device=device))\n",
        "eval_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Построение графиков по логам (matplotlib)\n",
        "import matplotlib.pyplot as plt\n",
        "csv_path = (log_dir / 'train_log.csv')\n",
        "print('CSV:', csv_path)\n",
        "from agentslab.utils.curves import plot_training_curves\n",
        "plot_training_curves(str(csv_path))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Запуск среды с выбранным render_mode\n",
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "from tqdm import tqdm\n",
        "\n",
        "if env_cfg.render_mode is None:\n",
        "    print('Пересоздаём env с render_mode=\"human\" для визуализации...')\n",
        "    env_cfg_vis = GymEnvConfig(env_id=env_cfg.env_id, render_mode='human', device=device, seed=seed)\n",
        "    env_vis = make_gym_env(env_cfg_vis)\n",
        "else:\n",
        "    env_vis = env\n",
        "\n",
        "with torch.no_grad():\n",
        "    with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "        td = env_vis.reset()\n",
        "        for _ in tqdm(range(500)):\n",
        "            td = actor(td)\n",
        "            td = env_vis.step(td)\n",
        "            td = td.get('next')\n",
        "\n",
        "env_vis.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Восстановление модели из чекпоинта\n",
        "from agentslab.utils.checkpointers import load_checkpoint\n",
        "loaded = load_checkpoint(str(Path(run_ckpt_dir) / 'checkpoint.pt'), actor, critic)\n",
        "print('Loaded extra:', loaded)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "marl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
