{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-agent PPO on VMAS Navigation (TorchRL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specs: Composite(\n",
      "    agents: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([64, 4, 18]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([64, 4, 18]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([64, 4, 18]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        info: Composite(\n",
      "            pos_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([64, 4, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            final_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([64, 4, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            agent_collisions: UnboundedContinuous(\n",
      "                shape=torch.Size([64, 4, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([64, 4, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            device=cpu,\n",
      "            shape=torch.Size([64, 4]),\n",
      "            data_cls=None),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([64, 4]),\n",
      "        data_cls=None),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([64]),\n",
      "    data_cls=None) Composite(\n",
      "    agents: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([64, 4, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([64, 4, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([64, 4, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([64, 4]),\n",
      "        data_cls=None),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([64]),\n",
      "    data_cls=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:905: UserWarning: frames_per_batch (6000) is not exactly divisible by the number of batched environments (64),  this results in more frames_per_batch per iteration that requested (6016). To silence this message, set the environment variable RL_WARNINGS to False.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\objectives\\ppo.py:384: DeprecationWarning: 'critic_coef' is deprecated and will be removed in torchrl v0.11. Please use 'critic_coeff' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\objectives\\ppo.py:450: DeprecationWarning: 'entropy_coef' is deprecated and will be removed in torchrl v0.11. Please use 'entropy_coeff' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Setting 'advantage' via the constructor is deprecated, use .set_keys(<key>='some_key') instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m policy = build_gaussian_policy_for_marl(policy_net, env.action_key, env.action_spec_unbatched)\n\u001b[32m     24\u001b[39m critic = make_value_net(obs_dim, device=policy_device)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m run, rewards = \u001b[43mtrain_marl_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../checkpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvmas/navigation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRun:\u001b[39m\u001b[33m'\u001b[39m, run)\n\u001b[32m     28\u001b[39m rewards\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\AgentsLab\\src\\agentslab\\runner\\train.py:105\u001b[39m, in \u001b[36mtrain_marl_ppo\u001b[39m\u001b[34m(env, policy, critic_value_net, device, log_dir, ckpt_dir, env_name, seed, cfg)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    103\u001b[39m collector = make_sync_collector(\u001b[38;5;28;01mlambda\u001b[39;00m: env, policy, cfg.frames_per_batch, cfg.frames_per_batch * cfg.n_iters, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m loss_module = \u001b[43mClipPPOLoss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor_network\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic_network\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcritic_value_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_epsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentropy_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic_coef\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_target_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue_target\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43madvantage_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madvantage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m loss_module.set_keys(\n\u001b[32m    115\u001b[39m     action=env.action_key,\n\u001b[32m    116\u001b[39m     reward=env.reward_key,\n\u001b[32m    117\u001b[39m     value=(\u001b[33m\"\u001b[39m\u001b[33magents\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstate_value\u001b[39m\u001b[33m\"\u001b[39m),      \u001b[38;5;66;03m# критик пишет сюда\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# sample_log_prob оставляем по умолчанию: 'sample_log_prob'\u001b[39;00m\n\u001b[32m    119\u001b[39m )\n\u001b[32m    120\u001b[39m loss_module.make_value_estimator(ValueEstimators.GAE, gamma=cfg.gamma, lmbda=cfg.lmbda)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\objectives\\ppo.py:1094\u001b[39m, in \u001b[36mClipPPOLoss.__init__\u001b[39m\u001b[34m(self, actor_network, critic_network, clip_epsilon, entropy_bonus, samples_mc_entropy, entropy_coeff, critic_coeff, loss_critic_type, normalize_advantage, normalize_advantage_exclude_dims, gamma, separate_losses, reduction, clip_value, device, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clip_value, \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m   1092\u001b[39m     clip_value = clip_epsilon \u001b[38;5;28;01mif\u001b[39;00m clip_value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentropy_bonus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentropy_bonus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43msamples_mc_entropy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples_mc_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcritic_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_critic_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_critic_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_advantage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize_advantage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_advantage_exclude_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize_advantage_exclude_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparate_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparate_losses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\objectives\\ppo.py:491\u001b[39m, in \u001b[36mPPOLoss.__init__\u001b[39m\u001b[34m(self, actor_network, critic_network, entropy_bonus, samples_mc_entropy, entropy_coeff, log_explained_variance, critic_coeff, loss_critic_type, normalize_advantage, normalize_advantage_exclude_dims, gamma, separate_losses, advantage_key, value_target_key, value_key, functional, actor, critic, reduction, clip_value, device, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(_GAMMA_LMBDA_DEPREC_ERROR)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_deprecated_ctor_keys\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43madvantage\u001b[49m\u001b[43m=\u001b[49m\u001b[43madvantage_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_target_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clip_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clip_value, \u001b[38;5;28mfloat\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\objectives\\common.py:225\u001b[39m, in \u001b[36mLossModule._set_deprecated_ctor_keys\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m via the constructor is deprecated, use .set_keys(<key>=\u001b[39m\u001b[33m'\u001b[39m\u001b[33msome_key\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         )\n",
      "\u001b[31mRuntimeError\u001b[39m: Setting 'advantage' via the constructor is deprecated, use .set_keys(<key>='some_key') instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tensordict.nn import set_composite_lp_aggregate\n",
    "from agentslab.envs.vmas import make_vmas_env\n",
    "from agentslab.models.networks import make_multiagent_mlp\n",
    "from agentslab.models.policy import build_gaussian_policy_for_marl\n",
    "from agentslab.models.value import make_value_net\n",
    "from agentslab.runner.train import train_marl_ppo, PPOConfig\n",
    "from agentslab.utils.device import select_device, split_devices\n",
    "from agentslab.utils.seeding import seed_everything\n",
    "\n",
    "policy_device = select_device('cuda')\n",
    "policy_device, vmas_device = split_devices(policy_device)\n",
    "seed_everything(0)\n",
    "\n",
    "env = make_vmas_env('navigation', num_envs=64, device=policy_device, vmas_device=vmas_device, seed=0)\n",
    "print('Specs:', env.observation_spec, env.action_spec)\n",
    "\n",
    "# Build decentralised policy and critic\n",
    "obs_dim = env.full_observation_spec['agents','observation'].shape[-1]\n",
    "n_agents = env.n_agents\n",
    "act_dim = env.full_action_spec[env.action_key].shape[-1]\n",
    "policy_net = make_multiagent_mlp(obs_dim, 2 * act_dim, n_agents=n_agents, device=policy_device, centralized=False, share_params=True)\n",
    "policy = build_gaussian_policy_for_marl(policy_net, env.action_key, env.action_spec_unbatched)\n",
    "critic = make_value_net(obs_dim, device=policy_device)\n",
    "\n",
    "run, rewards = train_marl_ppo(env, policy, critic, device=policy_device, log_dir='../logs', ckpt_dir='../checkpoints', env_name='vmas/navigation', seed=0, cfg=PPOConfig())\n",
    "print('Run:', run)\n",
    "rewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
