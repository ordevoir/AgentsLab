{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7a86d02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Общее конфигурирование\n",
        "from agentslab.utils.device import resolve_device\n",
        "from agentslab.utils.seeding import set_global_seed\n",
        "from pathlib import Path\n",
        "\n",
        "device = resolve_device(\"cpu\")\n",
        "print('Device:', device)\n",
        " \n",
        "seed = 42\n",
        "set_global_seed(seed, deterministic=True)\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "ENV_ID = \"InvertedDoublePendulum-v4\"\n",
        "# ENV_ID = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54dd996",
      "metadata": {},
      "source": [
        "# Создание среды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d131b0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:36: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
            "  warnings.warn(EXTENSION_WARNING)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m2025-08-22 21:44:08,295 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from agentslab.envs.gym_factory import GymEnvConfig, make_gym_env\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "env_cfg = GymEnvConfig(env_id=ENV_ID, render_mode=None, device=device, seed=seed)\n",
        "env = make_gym_env(env_cfg)\n",
        "check_env_specs(env)\n",
        "\n",
        "# from agentslab.utils.specs import print_specs\n",
        "# print_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c427be",
      "metadata": {},
      "source": [
        "# Создание актора и критика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3824baf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ValueOperator(\n",
              "    module=Sequential(\n",
              "      (0): Linear(in_features=11, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    ),\n",
              "    device=cpu,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['state_value'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.modules.networks import MLPConfig, build_mlp\n",
        "from agentslab.modules.policy import build_stochastic_actor\n",
        "from torchrl.modules import ValueOperator\n",
        "\n",
        "# Достаём размерности\n",
        "obs_dim = env.observation_spec[\"observation\"].shape[-1]\n",
        "act_dim = env.action_spec.shape[-1]\n",
        "\n",
        "mlp_cfg = MLPConfig(\n",
        "        in_dim = obs_dim, \n",
        "        out_dim = 2*act_dim,\n",
        "        hidden_sizes = (256, 256),\n",
        "        activation = \"tanh\",\n",
        "        layer_norm = False\n",
        ")\n",
        "\n",
        "actor_network = build_mlp(mlp_cfg)\n",
        "actor = build_stochastic_actor(actor_network, env.action_spec)\n",
        "\n",
        "mlp_cfg.out_dim = act_dim\n",
        "critic_network = build_mlp(mlp_cfg)\n",
        "critic = ValueOperator(module=critic_network, in_keys=[\"observation\"])\n",
        "critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebea77ec",
      "metadata": {},
      "source": [
        "# Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9cdc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "frames_per_batch = 1000\n",
        "# For a complete training, bring the number of frames up to 1M\n",
        "total_frames = 10_000\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    create_env_fn=env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    split_trajs=False,\n",
        "    device=device,\n",
        ")\n",
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8859fad",
      "metadata": {},
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e45ce2a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "import torch\n",
        "\n",
        "gamma = 0.99\n",
        "lmbda = 0.95\n",
        "\n",
        "advantage_module = GAE(\n",
        "    gamma=gamma, lmbda=lmbda, value_network=critic, average_gae=True\n",
        ")\n",
        "\n",
        "clip_epsilon = (\n",
        "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
        ")\n",
        "entropy_eps = 1e-4\n",
        "\n",
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_bonus=bool(entropy_eps),\n",
        "    entropy_coeff=entropy_eps,\n",
        "    # these keys match by default but we set this for completeness\n",
        "    critic_coeff=1.0,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        ")\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optim, \n",
        "    T_max=total_frames // frames_per_batch, \n",
        "    eta_min=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47ea80e",
      "metadata": {},
      "source": [
        "# Make Dirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd7972b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "import sys, csv, json, platform, re\n",
        "from typing import Dict, Any, Optional, List\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c16259",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Paths & meta ----------\n",
        "@dataclass\n",
        "class RunPaths:\n",
        "    runs_root_dir: Path\n",
        "    run_dir: Path\n",
        "    ckpt_dir: Path\n",
        "    csv_train: Path\n",
        "    csv_eval: Path\n",
        "    tb_train: Path\n",
        "    tb_eval: Path\n",
        "    meta_yaml: Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943cf5b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_run_paths(cfg) -> RunPaths:\n",
        "    run_name = f\"{cfg.algo_name}_{cfg.env_name}_{datetime.now().strftime(cfg.run_time_fmt)}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_run_name(algo: str, env: str) -> str:\n",
        "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # локальное время машины\n",
        "    safe_algo = re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"_\", algo)\n",
        "    safe_env  = re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"_\", env)\n",
        "    return f\"{safe_algo}_{safe_env}_{ts}\"\n",
        "\n",
        "\n",
        "def setup_run(\n",
        "    root: Path, algo: str, env: str, extra_meta: Optional[Dict[str, Any]] = None\n",
        ") -> tuple[str, RunPaths, \"CSVLogger\", \"CSVLogger\", SummaryWriter, SummaryWriter]:\n",
        "    run_name = make_run_name(algo, env)\n",
        "    run_dir  = root / \"runs\" / run_name\n",
        "\n",
        "    ckpt_dir = run_dir / \"checkpoints\"\n",
        "    csv_dir  = run_dir / \"csv_logs\"\n",
        "    tb_train = run_dir / \"tb_logs\" / \"train\"\n",
        "    tb_eval  = run_dir / \"tb_logs\" / \"eval\"\n",
        "    for d in (ckpt_dir, csv_dir, tb_train, tb_eval):\n",
        "        d.mkdirs = d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    paths = RunPaths(\n",
        "        root=root, \n",
        "        run_dir=run_dir, \n",
        "        ckpt_dir=ckpt_dir,\n",
        "        csv_train=csv_dir / \"train.csv\", \n",
        "        csv_eval=csv_dir / \"eval.csv\",\n",
        "        tb_train=tb_train, \n",
        "        tb_eval=tb_eval, \n",
        "        meta_yaml=run_dir / \"meta_info.yaml\"\n",
        "    )\n",
        "\n",
        "    # meta_info.yaml — минимально полезные поля\n",
        "    meta: Dict[str, Any] = {\n",
        "        \"algo_name\": algo,\n",
        "        \"env_name\": env,\n",
        "        \"run_name\": run_name,\n",
        "        \"started_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"platform\": platform.platform(),\n",
        "        \"torch\": torch.__version__,\n",
        "    }\n",
        "    if extra_meta:\n",
        "        meta.update(extra_meta)\n",
        "\n",
        "    try:\n",
        "        import yaml  # PyYAML\n",
        "        with open(paths.meta_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "            yaml.safe_dump(meta, f, sort_keys=False, allow_unicode=True)\n",
        "    except Exception:\n",
        "        # fallback на JSON рядом, если PyYAML не установлен\n",
        "        with open(paths.meta_yaml.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # CSV loggers\n",
        "    train_csv = CSVLogger(paths.csv_train, fieldnames=[\"frame\", \"reward_mean\", \"step_count_max\", \"lr\"])\n",
        "    eval_csv  = CSVLogger(paths.csv_eval,  fieldnames=[\"frame\", \"return_mean\", \"return_std\", \"max_episode_length\"])\n",
        "\n",
        "    # TensorBoard\n",
        "    tbw_train = SummaryWriter(paths.tb_train.as_posix())\n",
        "    tbw_eval  = SummaryWriter(paths.tb_eval.as_posix())\n",
        "\n",
        "    return run_name, paths, train_csv, eval_csv, tbw_train, tbw_eval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfa17c6",
      "metadata": {},
      "source": [
        "\n",
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae8fc8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------- CSV logger ----------\n",
        "class CSVLogger:\n",
        "    def __init__(self, filepath: Path, fieldnames: List[str]):\n",
        "        self.filepath = Path(filepath)\n",
        "        self.filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self._file = self.filepath.open(\"a\", newline=\"\", encoding=\"utf-8\")\n",
        "        self.writer = csv.DictWriter(self._file, fieldnames=fieldnames)\n",
        "        if self.filepath.stat().st_size == 0:\n",
        "            self.writer.writeheader()\n",
        "            self._file.flush()\n",
        "\n",
        "    def log(self, row: Dict[str, Any]) -> None:\n",
        "        self.writer.writerow(row)\n",
        "        self._file.flush()\n",
        "\n",
        "    def close(self) -> None:\n",
        "        try:\n",
        "            self._file.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# ---------- TensorBoard & CSV logging helpers ----------\n",
        "def log_train_step(tbw: SummaryWriter, csv_logger: CSVLogger, frame: int, reward_mean: float, step_count_max: int, lr: float) -> None:\n",
        "    tbw.add_scalar(\"train/reward_mean\",     reward_mean,    frame)\n",
        "    tbw.add_scalar(\"train/step_count_max\",  step_count_max, frame)\n",
        "    tbw.add_scalar(\"train/lr\",              lr,             frame)\n",
        "    csv_logger.log({\n",
        "        \"frame\": frame,\n",
        "        \"reward_mean\": reward_mean,\n",
        "        \"step_count_max\": step_count_max,\n",
        "        \"lr\": lr,\n",
        "    })\n",
        "\n",
        "\n",
        "def log_eval_step(tbw: SummaryWriter, csv_logger: CSVLogger, frame: int, eval_results: Dict[str, Any]) -> None:\n",
        "    # ожидаемые ключи: return_mean, return_std (если есть), max_episode_length / max_episode_lengh\n",
        "    ret_mean = float(eval_results.get(\"return_mean\", float(\"nan\")))\n",
        "    ret_std  = float(eval_results.get(\"return_std\", float(\"nan\")))\n",
        "    max_len  = int(eval_results.get(\"max_episode_length\", eval_results.get(\"max_episode_lengh\", -1)))\n",
        "\n",
        "    tbw.add_scalar(\"eval/return_mean\", ret_mean, frame)\n",
        "    if not (ret_std != ret_std):  # not NaN\n",
        "        tbw.add_scalar(\"eval/return_std\", ret_std, frame)\n",
        "    tbw.add_scalar(\"eval/max_episode_length\", max_len, frame)\n",
        "\n",
        "    csv_logger.log({\n",
        "        \"frame\": frame,\n",
        "        \"return_mean\": ret_mean,\n",
        "        \"return_std\": ret_std,\n",
        "        \"max_episode_length\": max_len,\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39743834",
      "metadata": {},
      "source": [
        "# Progress Bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758e6025",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_progress_bar(total_frames: int) -> tqdm:\n",
        "    return tqdm(total=total_frames, dynamic_ncols=True, leave=True)\n",
        "\n",
        "\n",
        "def update_progress_bar(pbar: tqdm, batch_frames: int, *, reward_mean: float, step_count_max: int, lr: float) -> None:\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "    pbar.set_description(\n",
        "        f\"avg reward={reward_mean: 4.4f}, max step count: {step_count_max}, lr: {lr: 4.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def close_run(train_csv: CSVLogger, eval_csv: CSVLogger, tb_train: SummaryWriter, tb_eval: SummaryWriter, pbar: Optional[tqdm] = None) -> None:\n",
        "    train_csv.close()\n",
        "    eval_csv.close()\n",
        "    tb_train.flush(); tb_train.close()\n",
        "    tb_eval.flush();  tb_eval.close()\n",
        "    if pbar is not None:\n",
        "        pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b67a6e",
      "metadata": {},
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e46dee",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CheckpointManager:\n",
        "    ckpt_dir: Path\n",
        "    metric_key: str = \"return_mean\"\n",
        "    mode: str = \"max\"  # \"max\" или \"min\"\n",
        "    best_value: Optional[float] = None\n",
        "    save_n_latest: Optional[int] = 10\n",
        "    _history: List[Path] = field(default_factory=list)\n",
        "\n",
        "    def _is_better(self, val: float) -> bool:\n",
        "        if self.best_value is None:\n",
        "            return True\n",
        "        return (val > self.best_value) if self.mode == \"max\" else (val < self.best_value)\n",
        "\n",
        "    def save_eval_ckpt(\n",
        "        self,\n",
        "        *,\n",
        "        actor,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        loss_module,\n",
        "        frame: int,\n",
        "        metrics: Dict[str, Any],\n",
        "        extra_state: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Сохраняет checkpoints/last.pt, checkpoints/step_{frame}.pt и при улучшении — best.pt\"\"\"\n",
        "        state = {\n",
        "            \"frame\": frame,\n",
        "            \"metrics\": metrics,\n",
        "            \"actor_state_dict\": actor.state_dict() if hasattr(actor, \"state_dict\") else None,\n",
        "            \"optimizer_state_dict\": optimizer.state_dict() if optimizer is not None else None,\n",
        "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
        "            \"loss_module_state_dict\": loss_module.state_dict() if loss_module is not None else None,\n",
        "            \"torch_version\": torch.__version__,\n",
        "        }\n",
        "        if extra_state:\n",
        "            state.update(extra_state)\n",
        "\n",
        "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        last_path = self.ckpt_dir / \"last.pt\"\n",
        "        step_path = self.ckpt_dir / f\"step_{frame}.pt\"\n",
        "        torch.save(state, last_path)\n",
        "        torch.save(state, step_path)\n",
        "        self._history.append(step_path)\n",
        "\n",
        "        # сдерживаем рост числа step_*.pt\n",
        "        if self.save_n_latest is not None and len(self._history) > self.save_n_latest:\n",
        "            old = self._history.pop(0)\n",
        "            try:\n",
        "                old.unlink(missing_ok=True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        result = {\"is_best\": False, \"best_value\": self.best_value}\n",
        "        val = metrics.get(self.metric_key, None)\n",
        "        if isinstance(val, (int, float)) and self._is_better(float(val)):\n",
        "            self.best_value = float(val)\n",
        "            torch.save(state, self.ckpt_dir / \"best.pt\")\n",
        "            result.update({\"is_best\": True, \"best_value\": self.best_value})\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c93040",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe88d15",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "CSVLogger.__init__() got an unexpected keyword argument 'log_dir'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m paths = make_run_dirs(ROOT, ALGO_NAME, ENV_NAME)\n\u001b[32m      2\u001b[39m txt_logs = setup_text_loggers(paths)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m mlogs = \u001b[43msetup_metric_loggers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# пример гиперпараметров/конфига (дополните своим)\u001b[39;00m\n\u001b[32m      6\u001b[39m hparams = \u001b[38;5;28mdict\u001b[39m(algo=ALGO_NAME, env=ENV_NAME, max_grad_norm=\u001b[32m1.0\u001b[39m, num_epochs=\u001b[32m10\u001b[39m, sub_batch_size=\u001b[32m64\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36msetup_metric_loggers\u001b[39m\u001b[34m(paths)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_metric_loggers\u001b[39m(paths: Dict[\u001b[38;5;28mstr\u001b[39m, Path]) -> Dict[\u001b[38;5;28mstr\u001b[39m, MultiLogger]:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# exp_name = имя файла CSV и подпапки TB, логи лежат напрямую в train/eval\u001b[39;00m\n\u001b[32m     49\u001b[39m     exp_name = paths[\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     train_csv = \u001b[43mCSVLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsv_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     eval_csv  = CSVLogger(log_dir=\u001b[38;5;28mstr\u001b[39m(paths[\u001b[33m\"\u001b[39m\u001b[33mcsv_eval\u001b[39m\u001b[33m\"\u001b[39m]),  exp_name=exp_name)\n\u001b[32m     52\u001b[39m     train_tb  = TensorboardLogger(log_dir=\u001b[38;5;28mstr\u001b[39m(paths[\u001b[33m\"\u001b[39m\u001b[33mtb_train\u001b[39m\u001b[33m\"\u001b[39m]), exp_name=exp_name)\n",
            "\u001b[31mTypeError\u001b[39m: CSVLogger.__init__() got an unexpected keyword argument 'log_dir'"
          ]
        }
      ],
      "source": [
        "# Параметры запуска\n",
        "from pathlib import Path\n",
        "ROOT = Path('..').resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "\n",
        "# Настройка run-папки, логгеров и чекпоинтов\n",
        "extra_meta = {\n",
        "    \"total_frames\": int(total_frames),\n",
        "    \"frames_per_batch\": int(frames_per_batch),\n",
        "    \"num_epochs\": int(10),\n",
        "    \"sub_batch_size\": int(64),\n",
        "    # при желании добавьте seed, гиперпараметры и пр.\n",
        "}\n",
        "run_name, paths, train_csv, eval_csv, tb_train, tb_eval = setup_run(ROOT, ALGO_NAME, ENV_NAME, extra_meta)\n",
        "ckpt_mgr = CheckpointManager(paths.ckpt_dir, metric_key=\"return_mean\", mode=\"max\", save_n_latest=10)\n",
        "\n",
        "# Прогресс-бар\n",
        "pbar = create_progress_bar(total_frames=total_frames)\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_epochs = 10\n",
        "sub_batch_size = 64\n",
        "\n",
        "# Итерируемся по коллекторам, пока не наберём нужное число шагов\n",
        "for i, tensordict_data in enumerate(collector):\n",
        "    # === TRAIN ===\n",
        "    for _ in range(num_epochs):\n",
        "        advantage_module(tensordict_data)\n",
        "        data_view = tensordict_data.reshape(-1)\n",
        "        replay_buffer.extend(data_view.cpu())\n",
        "\n",
        "        for _ in range(frames_per_batch // sub_batch_size):\n",
        "            subdata = replay_buffer.sample(sub_batch_size)\n",
        "            loss_vals = loss_module(subdata.to(device))\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"]\n",
        "                + loss_vals[\"loss_critic\"]\n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "            loss_value.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "    scheduler.step()  # Шаг LR-планировщика\n",
        "\n",
        "    # === ЛОГ TRAIN ===\n",
        "    reward_mean    = tensordict_data[\"next\", \"reward\"].mean().item()\n",
        "    step_count_max = tensordict_data[\"step_count\"].max().item()\n",
        "    lr_val         = float(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "    # корректное число фреймов в пачке\n",
        "    batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "\n",
        "    # обновляем прогресс-бар и получаем текущий глобальный frame\n",
        "    update_progress_bar(pbar, batch_frames, reward_mean=reward_mean, step_count_max=step_count_max, lr=lr_val)\n",
        "    global_frame = int(pbar.n)  # используем число обработанных кадров как глобальный шаг\n",
        "\n",
        "    # пишем train в TB+CSV (минимум шумных метрик)\n",
        "    log_train_step(tb_train, train_csv, global_frame, reward_mean, step_count_max, lr_val)\n",
        "\n",
        "    # === EVAL каждые 5 партий ===\n",
        "    if (i + 1) % 5 == 0:\n",
        "        eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "        # Логи eval\n",
        "        log_eval_step(tb_eval, eval_csv, global_frame, eval_results)\n",
        "\n",
        "        # Чекпоинты — только на eval\n",
        "        ckpt_info = ckpt_mgr.save_eval_ckpt(\n",
        "            actor=actor,\n",
        "            optimizer=optim,\n",
        "            scheduler=scheduler,\n",
        "            loss_module=loss_module,\n",
        "            frame=global_frame,\n",
        "            metrics=eval_results,\n",
        "            extra_state={\"run_name\": run_name},\n",
        "        )\n",
        "\n",
        "        # Сообщение в pbar, не ломает отрисовку\n",
        "        msg = (\n",
        "            f\"eval: avg reward = {eval_results.get('return_mean'):.4f}, \"\n",
        "            f\"max episode length = {eval_results.get('max_episode_length', eval_results.get('max_episode_lengh', -1))}\"\n",
        "        )\n",
        "        if ckpt_info.get(\"is_best\"):\n",
        "            msg += f\" | NEW BEST (return_mean={ckpt_info['best_value']:.4f}) → checkpoints/best.pt\"\n",
        "        pbar.write(msg)\n",
        "\n",
        "# Завершаем корректно\n",
        "close_run(train_csv, eval_csv, tb_train, tb_eval, pbar)\n",
        "print(f\"Run saved to: {paths.run_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875bff8b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6daa5cbb06634fb788b0a9cfe18b94e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval: avg reward = 154.026, max episode length = 21.0\n",
            "eval: avg reward = 142.631, max episode length = 16.0\n"
          ]
        }
      ],
      "source": [
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "global_frames = 0\n",
        "pbar = create_pbar(total_frames)\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_epochs = 10\n",
        "sub_batch_size = 64\n",
        "\n",
        "for i, tensordict_data in enumerate(collector):\n",
        "    # ----------- обучение на партии -----------------------------------------\n",
        "    for _ in range(num_epochs):\n",
        "        advantage_module(tensordict_data)\n",
        "        data_view = tensordict_data.reshape(-1)\n",
        "        replay_buffer.extend(data_view.cpu())\n",
        "        for _ in range(frames_per_batch // sub_batch_size):\n",
        "            subdata = replay_buffer.sample(sub_batch_size)\n",
        "            loss_vals = loss_module(subdata.to(device))\n",
        "            loss_value = (\n",
        "                loss_vals[\"loss_objective\"]\n",
        "                + loss_vals[\"loss_critic\"]\n",
        "                + loss_vals[\"loss_entropy\"]\n",
        "            )\n",
        "            loss_value.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # --- метрики train (минимальный набор; расширяйте при необходимости) -----\n",
        "    # средняя награда по текущей партии (если поле есть)\n",
        "    rew = tensordict_data.get((\"next\", \"reward\"))\n",
        "    avg_rew = float(rew.mean().item()) if rew is not None else None\n",
        "    # текущий lr\n",
        "    try:\n",
        "        lr_val = float(scheduler.get_last_lr()[0])\n",
        "    except Exception:\n",
        "        lr_val = None\n",
        "\n",
        "    # корректное число фреймов в пачке\n",
        "    batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "    global_frames += batch_frames\n",
        "\n",
        "    # логгируем train-скаляры\n",
        "    train_metrics = {\n",
        "        \"loss/objective\": float(loss_vals[\"loss_objective\"].item()),\n",
        "        \"loss/critic\": float(loss_vals[\"loss_critic\"].item()),\n",
        "        \"loss/entropy\": float(loss_vals[\"loss_entropy\"].item()),\n",
        "    }\n",
        "    if avg_rew is not None: train_metrics[\"reward/avg\"] = avg_rew\n",
        "    if lr_val is not None:  train_metrics[\"lr\"] = lr_val\n",
        "\n",
        "    for k, v in train_metrics.items():\n",
        "        mlogs[\"train\"].log_scalar(k, v, step=global_frames)\n",
        "    txt_logs[\"train\"].info(f\"step={global_frames} \" + \", \".join(f\"{k}={v:.6f}\" for k, v in train_metrics.items()))\n",
        "\n",
        "    # --- периодическая оценка -------------------------------------------------\n",
        "    if (i + 1) % 5 == 0:\n",
        "        eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "        # логируем eval-метрики\n",
        "        em = {\n",
        "            \"return_mean\": float(eval_results.get(\"return_mean\", float(\"nan\"))),\n",
        "            \"max_episode_length\": float(eval_results.get(\"max_episode_lengh\", float(\"nan\"))),\n",
        "        }\n",
        "        for k, v in em.items():\n",
        "            mlogs[\"eval\"].log_scalar(k, v, step=global_frames)\n",
        "        txt_logs[\"eval\"].info(f\"step={global_frames} \" + \", \".join(f\"{k}={v:.6f}\" for k, v in em.items()))\n",
        "\n",
        "        # печать поверх прогресс-бара (не ломает отрисовку)\n",
        "        pbar.write(f\"eval: avg reward = {em['return_mean']:.3f}, max episode length = {em['max_episode_length']:.1f}\")\n",
        "\n",
        "        # чекпоинт во время eval (и last, и best)\n",
        "        ckpt_mgr.maybe_save(step=global_frames, eval_metrics=em, actor=actor, optim=optim, scheduler=scheduler)\n",
        "\n",
        "    # --- прогресс-бар ---------------------------------------------------------\n",
        "    update_pbar(pbar, batch_frames, avg_reward=avg_rew, step_count=None, lr=lr_val)\n",
        "\n",
        "# гарантированно закрываем бар и логгеры\n",
        "pbar.close()\n",
        "mlogs[\"train\"].close(); mlogs[\"eval\"].close()\n",
        "for lg in txt_logs.values():\n",
        "    for h in list(lg.handlers):\n",
        "        h.flush(); h.close(); lg.removeHandler(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f68cfcc",
      "metadata": {},
      "source": [
        "# Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7abc3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using run_dir: C:\\Users\\ordevoir\\Documents\\GitHub\\AgentsLab\\runs\\ppo_pendulum_20250822-202407\n",
            "Train columns: []\n",
            "Eval columns: []\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'step'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# === Примеры графиков ===\u001b[39;00m\n\u001b[32m     53\u001b[39m plt.figure()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m plt.plot(\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df_train.get(\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m, pd.Series([\u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m)]*\u001b[38;5;28mlen\u001b[39m(df_train))))\n\u001b[32m     55\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mTrain: average reward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mglobal frames (step)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ordevoir\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'step'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === plotting_example.py (запускать в новой ячейке после обучения) ===\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Укажите путь к последнему ран-каналу, либо найдите автоматически:\n",
        "ROOT = Path(\"..\").resolve()\n",
        "runs_dir = ROOT / \"runs\"\n",
        "\n",
        "# Найдём последний ран (по времени модификации папки)\n",
        "run_dirs = sorted([p for p in runs_dir.glob(\"*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "assert len(run_dirs) > 0, \"Не найдено ни одного запуска в runs/\"\n",
        "run_dir = run_dirs[0]\n",
        "print(\"Using run_dir:\", run_dir)\n",
        "\n",
        "def _load_pl_csv(csv_root: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    CSVLogger из Lightning создаёт иерархию: <csv_root>/<name>/version_x/metrics.csv\n",
        "    и может писать в «длинном» формате (name, step, value) либо «широком» (step + метрики).\n",
        "    Эта функция аккуратно приводить к широкому формату: столбцы — метрики, индекс — step.\n",
        "    \"\"\"\n",
        "    # ищем все варианты metrics.csv\n",
        "    metrics_files = list(csv_root.glob(\"**/metrics.csv\"))\n",
        "    if not metrics_files:\n",
        "        return pd.DataFrame()\n",
        "    # берём последний по времени\n",
        "    metrics_path = sorted(metrics_files, key=lambda p: p.stat().st_mtime)[-1]\n",
        "    df = pd.read_csv(metrics_path)\n",
        "\n",
        "    if {\"name\", \"step\", \"value\"}.issubset(df.columns):\n",
        "        # длинный формат -> pivot\n",
        "        wide = df.pivot_table(index=\"step\", columns=\"name\", values=\"value\", aggfunc=\"last\")\n",
        "        wide.sort_index(inplace=True)\n",
        "        wide.reset_index(inplace=True)\n",
        "        return wide\n",
        "    else:\n",
        "        # уже широкий формат\n",
        "        if \"step\" not in df.columns:\n",
        "            # если step отсутствует — добавим монотонный по индексу\n",
        "            df.insert(0, \"step\", range(len(df)))\n",
        "        return df\n",
        "\n",
        "train_csv_root = run_dir / \"csv_logs\" / \"train\"\n",
        "eval_csv_root  = run_dir / \"csv_logs\" / \"eval\"\n",
        "\n",
        "df_train = _load_pl_csv(train_csv_root)\n",
        "df_eval  = _load_pl_csv(eval_csv_root)\n",
        "\n",
        "print(\"Train columns:\", df_train.columns.tolist())\n",
        "print(\"Eval columns:\", df_eval.columns.tolist())\n",
        "\n",
        "# === Примеры графиков ===\n",
        "plt.figure()\n",
        "plt.plot(df_train[\"step\"], df_train.get(\"reward\", pd.Series([float(\"nan\")]*len(df_train))))\n",
        "plt.title(\"Train: average reward\")\n",
        "plt.xlabel(\"global frames (step)\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "if \"loss_total\" in df_train.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_train[\"step\"], df_train[\"loss_total\"])\n",
        "    plt.title(\"Train: total loss\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"loss_total\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if not df_eval.empty and \"return_mean\" in df_eval.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_eval[\"step\"], df_eval[\"return_mean\"], marker=\"o\")\n",
        "    plt.title(\"Eval: return_mean\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"return_mean\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea88a1de",
      "metadata": {},
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ade9f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91d7dbc95013412e9eda87a6b073c091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "eval:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'return_mean': 179.49685134887696,\n",
              " 'return_sum': 8974.842567443848,\n",
              " 'max_episode_lengh': 33,\n",
              " 'num_episodes': 50}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "eval_policy(env, actor, episodes=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f394edd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.auto import tqdm\n",
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "class TrainingLogger:\n",
        "    \"\"\"Класс для управления логгированием тренировки\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir: Path, algo_name: str, env_name: str):\n",
        "        self.root_dir = root_dir\n",
        "        self.algo_name = algo_name\n",
        "        self.env_name = env_name\n",
        "        \n",
        "        # Создание структуры директорий\n",
        "        self.run_dir = self._create_run_directory()\n",
        "        self.csv_train_dir = self.run_dir / \"csv_logs\" / \"train\"\n",
        "        self.csv_eval_dir = self.run_dir / \"csv_logs\" / \"eval\"\n",
        "        self.txt_train_dir = self.run_dir / \"txt_logs\" / \"train\"\n",
        "        self.txt_eval_dir = self.run_dir / \"txt_logs\" / \"eval\"\n",
        "        self.tb_train_dir = self.run_dir / \"tb_logs\" / \"train\"\n",
        "        self.tb_eval_dir = self.run_dir / \"tb_logs\" / \"eval\"\n",
        "        self.checkpoints_dir = self.run_dir / \"checkpoints\"\n",
        "        \n",
        "        # Создание всех директорий\n",
        "        for dir_path in [self.csv_train_dir, self.csv_eval_dir, self.txt_train_dir, \n",
        "                        self.txt_eval_dir, self.tb_train_dir, self.tb_eval_dir, \n",
        "                        self.checkpoints_dir]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Инициализация логгеров\n",
        "        self.tb_train_writer = SummaryWriter(self.tb_train_dir)\n",
        "        self.tb_eval_writer = SummaryWriter(self.tb_eval_dir)\n",
        "        \n",
        "        # CSV файлы\n",
        "        self.train_csv_path = self.csv_train_dir / \"training_logs.csv\"\n",
        "        self.eval_csv_path = self.csv_eval_dir / \"eval_logs.csv\"\n",
        "        \n",
        "        # Текстовые файлы\n",
        "        self.train_txt_path = self.txt_train_dir / \"training.log\"\n",
        "        self.eval_txt_path = self.txt_eval_dir / \"eval.log\"\n",
        "        \n",
        "        # Инициализация CSV файлов\n",
        "        self._init_csv_files()\n",
        "        \n",
        "        # Создание метаинформации\n",
        "        self._create_metadata()\n",
        "    \n",
        "    def _create_run_directory(self) -> Path:\n",
        "        \"\"\"Создание директории для запуска с временной меткой\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        run_name = f\"{self.algo_name}_{self.env_name}_{timestamp}\"\n",
        "        run_dir = self.root_dir / \"runs\" / run_name\n",
        "        run_dir.mkdir(parents=True, exist_ok=True)\n",
        "        return run_dir\n",
        "    \n",
        "    def _init_csv_files(self):\n",
        "        \"\"\"Инициализация CSV файлов с заголовками\"\"\"\n",
        "        # Заголовки для тренировочных логов\n",
        "        train_headers = ['iteration', 'reward', 'step_count', 'lr', 'loss_objective', \n",
        "                        'loss_critic', 'loss_entropy', 'total_loss']\n",
        "        with open(self.train_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(train_headers)\n",
        "        \n",
        "        # Заголовки для логов оценки\n",
        "        eval_headers = ['iteration', 'return_mean', 'return_std', 'max_episode_length', \n",
        "                       'min_episode_length', 'episodes']\n",
        "        with open(self.eval_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(eval_headers)\n",
        "    \n",
        "    def _create_metadata(self):\n",
        "        \"\"\"Создание YAML файла с метаинформацией\"\"\"\n",
        "        metadata = {\n",
        "            'algorithm': self.algo_name,\n",
        "            'environment': self.env_name,\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'run_directory': str(self.run_dir),\n",
        "            'structure': {\n",
        "                'csv_logs': ['train/', 'eval/'],\n",
        "                'txt_logs': ['train/', 'eval/'],\n",
        "                'tb_logs': ['train/', 'eval/'],\n",
        "                'checkpoints': []\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        metadata_path = self.run_dir / \"metadata.yaml\"\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            yaml.dump(metadata, f, default_flow_style=False)\n",
        "    \n",
        "    def log_training_step(self, iteration: int, logs: Dict[str, Any], \n",
        "                         loss_vals: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Логгирование шага тренировки\"\"\"\n",
        "        # CSV логгирование\n",
        "        with open(self.train_csv_path, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                iteration,\n",
        "                logs['reward'][-1],\n",
        "                logs['step_count'][-1],\n",
        "                logs['lr'][-1],\n",
        "                loss_vals[\"loss_objective\"].item(),\n",
        "                loss_vals[\"loss_critic\"].item(),\n",
        "                loss_vals[\"loss_entropy\"].item(),\n",
        "                (loss_vals[\"loss_objective\"] + loss_vals[\"loss_critic\"] + loss_vals[\"loss_entropy\"]).item()\n",
        "            ])\n",
        "        \n",
        "        # TensorBoard логгирование\n",
        "        self.tb_train_writer.add_scalar('Reward/Average', logs['reward'][-1], iteration)\n",
        "        self.tb_train_writer.add_scalar('Step_Count/Max', logs['step_count'][-1], iteration)\n",
        "        self.tb_train_writer.add_scalar('Learning_Rate', logs['lr'][-1], iteration)\n",
        "        self.tb_train_writer.add_scalar('Loss/Objective', loss_vals[\"loss_objective\"].item(), iteration)\n",
        "        self.tb_train_writer.add_scalar('Loss/Critic', loss_vals[\"loss_critic\"].item(), iteration)\n",
        "        self.tb_train_writer.add_scalar('Loss/Entropy', loss_vals[\"loss_entropy\"].item(), iteration)\n",
        "        \n",
        "        # Текстовое логгирование\n",
        "        log_message = (f\"Iteration {iteration}: reward={logs['reward'][-1]:.4f}, \"\n",
        "                      f\"step_count={logs['step_count'][-1]}, lr={logs['lr'][-1]:.4f}\")\n",
        "        with open(self.train_txt_path, 'a') as f:\n",
        "            f.write(f\"{datetime.now().isoformat()} - {log_message}\\n\")\n",
        "    \n",
        "    def log_evaluation(self, iteration: int, eval_results: Dict[str, Any]):\n",
        "        \"\"\"Логгирование результатов оценки\"\"\"\n",
        "        # CSV логгирование\n",
        "        with open(self.eval_csv_path, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                iteration,\n",
        "                eval_results.get(\"return_mean\", 0),\n",
        "                eval_results.get(\"return_std\", 0),\n",
        "                eval_results.get(\"max_episode_length\", 0),\n",
        "                eval_results.get(\"min_episode_length\", 0),\n",
        "                eval_results.get(\"episodes\", 5)\n",
        "            ])\n",
        "        \n",
        "        # TensorBoard логгирование\n",
        "        self.tb_eval_writer.add_scalar('Evaluation/Return_Mean', \n",
        "                                      eval_results.get(\"return_mean\", 0), iteration)\n",
        "        self.tb_eval_writer.add_scalar('Evaluation/Return_Std', \n",
        "                                      eval_results.get(\"return_std\", 0), iteration)\n",
        "        self.tb_eval_writer.add_scalar('Evaluation/Max_Episode_Length', \n",
        "                                      eval_results.get(\"max_episode_length\", 0), iteration)\n",
        "        \n",
        "        # Текстовое логгирование\n",
        "        log_message = (f\"Evaluation {iteration}: return_mean={eval_results.get('return_mean', 0):.4f}, \"\n",
        "                      f\"max_episode_length={eval_results.get('max_episode_length', 0)}\")\n",
        "        with open(self.eval_txt_path, 'a') as f:\n",
        "            f.write(f\"{datetime.now().isoformat()} - {log_message}\\n\")\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Закрытие логгеров\"\"\"\n",
        "        self.tb_train_writer.close()\n",
        "        self.tb_eval_writer.close()\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Класс для управления чекпоинтами\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoints_dir: Path):\n",
        "        self.checkpoints_dir = checkpoints_dir\n",
        "        self.best_reward = float('-inf')\n",
        "    \n",
        "    def save_checkpoint(self, iteration: int, model_state: Dict[str, Any], \n",
        "                       eval_results: Dict[str, Any], is_best: bool = False):\n",
        "        \"\"\"Сохранение чекпоинта\"\"\"\n",
        "        checkpoint = {\n",
        "            'iteration': iteration,\n",
        "            'model_state_dict': model_state,\n",
        "            'eval_results': eval_results,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        # Обычный чекпоинт\n",
        "        checkpoint_path = self.checkpoints_dir / f\"checkpoint_iter_{iteration}.pt\"\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        \n",
        "        # Лучший чекпоинт\n",
        "        current_reward = eval_results.get(\"return_mean\", float('-inf'))\n",
        "        if current_reward > self.best_reward:\n",
        "            self.best_reward = current_reward\n",
        "            best_checkpoint_path = self.checkpoints_dir / \"best_checkpoint.pt\"\n",
        "            torch.save(checkpoint, best_checkpoint_path)\n",
        "        \n",
        "        # Последний чекпоинт\n",
        "        latest_checkpoint_path = self.checkpoints_dir / \"latest_checkpoint.pt\"\n",
        "        torch.save(checkpoint, latest_checkpoint_path)\n",
        "\n",
        "class ProgressTracker:\n",
        "    \"\"\"Класс для управления прогресс-баром\"\"\"\n",
        "    \n",
        "    def __init__(self, total_frames: int):\n",
        "        self.pbar = tqdm(total=total_frames, desc=\"Training\")\n",
        "        self.total_frames = total_frames\n",
        "    \n",
        "    def update_progress(self, batch_frames: int):\n",
        "        \"\"\"Обновление прогресс-бара\"\"\"\n",
        "        inc = min(batch_frames, self.pbar.total - self.pbar.n)\n",
        "        if inc > 0:\n",
        "            self.pbar.update(inc)\n",
        "    \n",
        "    def update_description(self, logs: Dict[str, Any]):\n",
        "        \"\"\"Обновление описания прогресс-бара\"\"\"\n",
        "        avg_reward_str = f\"avg reward={logs['reward'][-1]:4.4f}\"\n",
        "        step_count_str = f\"max step count: {logs['step_count'][-1]}\"\n",
        "        lr_str = f\"lr: {logs['lr'][-1]:4.4f}\"\n",
        "        self.pbar.set_description(\", \".join([avg_reward_str, step_count_str, lr_str]))\n",
        "    \n",
        "    def write(self, message: str):\n",
        "        \"\"\"Вывод сообщения без нарушения прогресс-бара\"\"\"\n",
        "        self.pbar.write(message)\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Закрытие прогресс-бара\"\"\"\n",
        "        self.pbar.close()\n",
        "\n",
        "# Обновленный training loop\n",
        "def train_ppo_with_logging(collector, loss_module, advantage_module, replay_buffer, \n",
        "                          optim, scheduler, env, actor, device, logs, total_frames):\n",
        "    \"\"\"Основной цикл тренировки с логгированием\"\"\"\n",
        "    \n",
        "    # Параметры\n",
        "    max_grad_norm = 1.0\n",
        "    num_epochs = 10\n",
        "    sub_batch_size = 64\n",
        "    frames_per_batch = 1000  # Предполагаемое значение, замените на актуальное\n",
        "    \n",
        "    # Инициализация компонентов логгирования\n",
        "    logger = TrainingLogger(ROOT, ALGO_NAME, ENV_NAME)\n",
        "    checkpoint_manager = CheckpointManager(logger.checkpoints_dir)\n",
        "    progress_tracker = ProgressTracker(total_frames)\n",
        "    \n",
        "    try:\n",
        "        # Основной цикл тренировки\n",
        "        for i, tensordict_data in enumerate(collector):\n",
        "            # Тренировка на партии данных\n",
        "            total_loss_vals = None\n",
        "            for epoch in range(num_epochs):\n",
        "                advantage_module(tensordict_data)\n",
        "                data_view = tensordict_data.reshape(-1)\n",
        "                replay_buffer.extend(data_view.cpu())\n",
        "                \n",
        "                for batch_idx in range(frames_per_batch // sub_batch_size):\n",
        "                    subdata = replay_buffer.sample(sub_batch_size)\n",
        "                    loss_vals = loss_module(subdata.to(device))\n",
        "                    loss_value = (\n",
        "                        loss_vals[\"loss_objective\"] \n",
        "                        + loss_vals[\"loss_critic\"] \n",
        "                        + loss_vals[\"loss_entropy\"]\n",
        "                    )\n",
        "                    \n",
        "                    # Оптимизация\n",
        "                    loss_value.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
        "                    optim.step()\n",
        "                    optim.zero_grad()\n",
        "                    \n",
        "                    # Сохраняем последние значения лоссов для логгирования\n",
        "                    total_loss_vals = loss_vals\n",
        "            \n",
        "            scheduler.step()\n",
        "            \n",
        "            # Логгирование тренировки\n",
        "            if total_loss_vals is not None:\n",
        "                logger.log_training_step(i, logs, total_loss_vals)\n",
        "            \n",
        "            # Оценка и чекпоинты после каждых 5 партий\n",
        "            if (i + 1) % 5 == 0:\n",
        "                eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "                \n",
        "                # Логгирование оценки\n",
        "                logger.log_evaluation(i, eval_results)\n",
        "                \n",
        "                # Создание чекпоинта\n",
        "                model_state = {\n",
        "                    'loss_module': loss_module.state_dict(),\n",
        "                    'optimizer': optim.state_dict(),\n",
        "                    'scheduler': scheduler.state_dict()\n",
        "                }\n",
        "                checkpoint_manager.save_checkpoint(i, model_state, eval_results)\n",
        "                \n",
        "                # Вывод результатов оценки\n",
        "                progress_tracker.write(\n",
        "                    f\"eval: avg reward = {eval_results['return_mean']}, \"\n",
        "                    f\"max episode length = {eval_results.get('max_episode_length', 'N/A')}\"\n",
        "                )\n",
        "            \n",
        "            # Обновление прогресс-бара\n",
        "            batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "            progress_tracker.update_progress(batch_frames)\n",
        "            progress_tracker.update_description(logs)\n",
        "    \n",
        "    finally:\n",
        "        # Гарантированное закрытие всех компонентов\n",
        "        progress_tracker.close()\n",
        "        logger.close()\n",
        "\n",
        "# Использование:\n",
        "# train_ppo_with_logging(collector, loss_module, advantage_module, replay_buffer, \n",
        "#                       optim, scheduler, env, actor, device, logs, total_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcf8dc6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "marl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
