{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7a86d02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Общее конфигурирование\n",
        "from agentslab.utils.device import resolve_device\n",
        "from agentslab.utils.seeding import set_global_seed\n",
        "from pathlib import Path\n",
        "\n",
        "device = resolve_device(\"cpu\")\n",
        "print('Device:', device)\n",
        " \n",
        "seed = 42\n",
        "set_global_seed(seed, deterministic=True)\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "ENV_ID = \"InvertedDoublePendulum-v4\"\n",
        "# ENV_ID = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54dd996",
      "metadata": {},
      "source": [
        "# Создание среды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d131b0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m2025-08-23 01:10:44,428 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from agentslab.envs.gym_factory import GymEnvConfig, make_gym_env\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "env_cfg = GymEnvConfig(env_id=ENV_ID, render_mode=None, device=device, seed=seed)\n",
        "env = make_gym_env(env_cfg)\n",
        "check_env_specs(env)\n",
        "\n",
        "# from agentslab.utils.specs import print_specs\n",
        "# print_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c427be",
      "metadata": {},
      "source": [
        "# Создание актора и критика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3824baf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ValueOperator(\n",
              "    module=Sequential(\n",
              "      (0): Linear(in_features=11, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    ),\n",
              "    device=cpu,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['state_value'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.modules.networks import MLPConfig, build_mlp\n",
        "from agentslab.modules.policy import build_stochastic_actor\n",
        "from torchrl.modules import ValueOperator\n",
        "\n",
        "# Достаём размерности\n",
        "obs_dim = env.observation_spec[\"observation\"].shape[-1]\n",
        "act_dim = env.action_spec.shape[-1]\n",
        "\n",
        "mlp_cfg = MLPConfig(\n",
        "        in_dim = obs_dim, \n",
        "        out_dim = 2*act_dim,\n",
        "        hidden_sizes = (256, 256),\n",
        "        activation = \"tanh\",\n",
        "        layer_norm = False\n",
        ")\n",
        "\n",
        "actor_network = build_mlp(mlp_cfg)\n",
        "actor = build_stochastic_actor(actor_network, env.action_spec)\n",
        "\n",
        "mlp_cfg.out_dim = act_dim\n",
        "critic_network = build_mlp(mlp_cfg)\n",
        "critic = ValueOperator(module=critic_network, in_keys=[\"observation\"])\n",
        "critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebea77ec",
      "metadata": {},
      "source": [
        "# Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9cdc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "frames_per_batch = 1000\n",
        "# For a complete training, bring the number of frames up to 1M\n",
        "total_frames = 10_000\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    create_env_fn=env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    split_trajs=False,\n",
        "    device=device,\n",
        ")\n",
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8859fad",
      "metadata": {},
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e45ce2a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "import torch\n",
        "\n",
        "gamma = 0.99\n",
        "lmbda = 0.95\n",
        "\n",
        "advantage_module = GAE(\n",
        "    gamma=gamma, lmbda=lmbda, value_network=critic, average_gae=True\n",
        ")\n",
        "\n",
        "clip_epsilon = (\n",
        "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
        ")\n",
        "entropy_eps = 1e-4\n",
        "\n",
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_bonus=bool(entropy_eps),\n",
        "    entropy_coeff=entropy_eps,\n",
        "    # these keys match by default but we set this for completeness\n",
        "    critic_coeff=1.0,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        ")\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optim, \n",
        "    T_max=total_frames // frames_per_batch, \n",
        "    eta_min=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f0b4e9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0e651e",
      "metadata": {},
      "source": [
        "# Generate Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "16a231fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RunPaths:\n",
        "    root: Path\n",
        "    run_dir: Path\n",
        "    ckpt_dir: Path\n",
        "    csv_train: Path\n",
        "    csv_eval: Path\n",
        "    tb_train: Path\n",
        "    tb_eval: Path\n",
        "    meta_yaml: Path\n",
        "\n",
        "\n",
        "def generate_paths(root: Path, algo_name: str, env_name: str) -> RunPaths:\n",
        "    \"\"\"\n",
        "    Формирует пути по схеме:\n",
        "    root/runs/<algo>_<env>_<YYYYMMDD_HHMMSS>/\n",
        "    Директории не создаёт — только возвращает объект RunPaths.\n",
        "    \"\"\"\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_name = f\"{algo_name}_{env_name}_{ts}\"\n",
        "    run_dir = root / \"runs\" / run_name\n",
        "\n",
        "    # На случай редкого совпадения по секунде — гарантируем уникальность\n",
        "    if run_dir.exists():\n",
        "        i = 2\n",
        "        while (run_dir / f\"{run_name}__{i}\").exists():\n",
        "            i += 1\n",
        "        run_dir = run_dir / f\"{run_name}__{i}\"\n",
        "\n",
        "    return RunPaths(\n",
        "        root=root,\n",
        "        run_dir=run_dir,\n",
        "        ckpt_dir=run_dir / \"checkpoints\",\n",
        "        csv_train=run_dir / \"csv_logs\" / \"train.csv\",\n",
        "        csv_eval=run_dir / \"csv_logs\" / \"eval.csv\",\n",
        "        tb_train=run_dir / \"tb_logs\" / \"train\",\n",
        "        tb_eval=run_dir / \"tb_logs\" / \"eval\",\n",
        "        meta_yaml=run_dir / \"meta_info.yaml\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f6fa177",
      "metadata": {},
      "source": [
        "# CSV Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "baa47dfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Mapping, Union, Optional, List\n",
        "from numbers import Real\n",
        "import csv\n",
        "\n",
        "class CSVLogger:\n",
        "    \"\"\"\n",
        "    Простой и безопасный CSV-логгер.\n",
        "\n",
        "    Поведение:\n",
        "      - Заголовок берётся из ключей первой записи (порядок колонок сохраняется).\n",
        "      - Каждая следующая запись должна иметь тот же набор колонок.\n",
        "      - Родительские директории создаются автоматически.\n",
        "      - Если файл существует и непустой, схема читается из его заголовка.\n",
        "\n",
        "    Ограничения:\n",
        "      - Значения должны быть числами (int/float). Булевы значения не допускаются.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path: Union[str, Path]) -> None:\n",
        "        self.path = Path(csv_path)\n",
        "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self._fieldnames: Optional[List[str]] = None\n",
        "        self._header_written: bool = False\n",
        "\n",
        "        # Если файл уже есть и не пустой — считываем заголовок как схему\n",
        "        if self.path.exists() and self.path.stat().st_size > 0:\n",
        "            with self.path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f)\n",
        "                try:\n",
        "                    header = next(reader)\n",
        "                except StopIteration:\n",
        "                    header = None\n",
        "            if header:\n",
        "                self._fieldnames = list(header)\n",
        "                self._header_written = True\n",
        "\n",
        "    @property\n",
        "    def fieldnames(self) -> Optional[List[str]]:\n",
        "        \"\"\"Текущая фиксированная схема колонок (или None до первой записи).\"\"\"\n",
        "        return None if self._fieldnames is None else list(self._fieldnames)\n",
        "\n",
        "    def log(self, row: Mapping[str, Real]) -> None:\n",
        "        \"\"\"\n",
        "        Записать одну строку в CSV.\n",
        "        :param row: словарь {имя_колонки: числовое_значение}\n",
        "        \"\"\"\n",
        "        if not row:\n",
        "            raise ValueError(\"row пуст: нет данных для записи\")\n",
        "\n",
        "        # Проверка ключей и значений\n",
        "        for k, v in row.items():\n",
        "            if not isinstance(k, str):\n",
        "                raise TypeError(f\"Имя колонки должно быть str, получено: {type(k).__name__}\")\n",
        "            # bool является подклассом int — явно запретим, чтобы не путать с метриками\n",
        "            if isinstance(v, bool) or not isinstance(v, Real):\n",
        "                raise TypeError(f\"Значение в колонке '{k}' должно быть числом (int/float), получено: {type(v).__name__}\")\n",
        "\n",
        "        # Инициализируем схему при первой записи\n",
        "        if self._fieldnames is None:\n",
        "            self._fieldnames = list(row.keys())\n",
        "\n",
        "        # Проверяем согласованность схемы\n",
        "        row_keys = set(row.keys())\n",
        "        schema_keys = set(self._fieldnames)\n",
        "        if row_keys != schema_keys:\n",
        "            missing = schema_keys - row_keys\n",
        "            extra = row_keys - schema_keys\n",
        "            parts = []\n",
        "            if missing:\n",
        "                parts.append(f\"отсутствуют колонки: {sorted(missing)}\")\n",
        "            if extra:\n",
        "                parts.append(f\"лишние колонки: {sorted(extra)}\")\n",
        "            hint = \"; \".join(parts)\n",
        "            raise ValueError(f\"Схема не совпадает с заголовком CSV: {hint}\")\n",
        "\n",
        "        # Готовим упорядоченную запись в соответствии со схемой\n",
        "        ordered_row = {k: row[k] for k in self._fieldnames}\n",
        "\n",
        "        # Пишем (append), при необходимости — заголовок\n",
        "        write_header_now = not self._header_written\n",
        "        with self.path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=self._fieldnames)\n",
        "            if write_header_now:\n",
        "                writer.writeheader()\n",
        "                self._header_written = True\n",
        "            writer.writerow(ordered_row)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"CSVLogger(path={self.path!s}, fieldnames={self._fieldnames})\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca1d9cbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = generate_paths(ROOT, ALGO_NAME, ENV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef93a9a",
      "metadata": {},
      "source": [
        "# TB Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3cd06f66",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Mapping, Optional, Union\n",
        "from numbers import Real\n",
        "\n",
        "class TBLogger:\n",
        "    \"\"\"\n",
        "    Логгер для TensorBoard.\n",
        "\n",
        "    Конструктор:\n",
        "      TBLogger(log_dir, flush_secs=10, tag_prefix=\"\")\n",
        "        - log_dir: каталог, куда писать события TB (будет создан при необходимости)\n",
        "        - flush_secs: период авто-сброса буфера SummaryWriter\n",
        "        - tag_prefix: необязательный префикс для всех тэгов (напр. \"train/\")\n",
        "\n",
        "    Методы:\n",
        "      - log(row: Mapping[str, Real], step: Optional[int] = None)\n",
        "      - flush()\n",
        "      - close()\n",
        "      - контекстный менеджер (with TBLogger(...) as tb: ...)\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir: Union[str, Path], flush_secs: int = 10, tag_prefix: str = \"\") -> None:\n",
        "        self.log_dir = Path(log_dir)\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.tag_prefix = (tag_prefix.rstrip(\"/\") + \"/\") if tag_prefix else \"\"\n",
        "        self._step = 0\n",
        "\n",
        "        # Пытаемся импортировать лучший доступный SummaryWriter\n",
        "        writer_cls = None\n",
        "        try:\n",
        "            from torch.utils.tensorboard import SummaryWriter as _SW\n",
        "            writer_cls = _SW\n",
        "        except Exception:\n",
        "            try:\n",
        "                from tensorboardX import SummaryWriter as _SW  # type: ignore\n",
        "                writer_cls = _SW\n",
        "            except Exception as e:\n",
        "                raise ImportError(\n",
        "                    \"TBLogger требует либо PyTorch (torch.utils.tensorboard), либо tensorboardX.\"\n",
        "                ) from e\n",
        "\n",
        "        self._writer = writer_cls(log_dir=str(self.log_dir), flush_secs=flush_secs)\n",
        "\n",
        "    def log(self, row: Mapping[str, Real], step: Optional[int] = None) -> None:\n",
        "        \"\"\"\n",
        "        Записать набор скалярных метрик.\n",
        "        :param row: словарь {тег: число}\n",
        "        :param step: явный global_step; если None — используется внутренний счётчик и авто-инкремент\n",
        "        \"\"\"\n",
        "        if not row:\n",
        "            raise ValueError(\"row пуст: нет данных для записи в TensorBoard\")\n",
        "\n",
        "        # Валидация ключей/значений\n",
        "        for k, v in row.items():\n",
        "            if not isinstance(k, str):\n",
        "                raise TypeError(f\"Имя тэга должно быть str, получено {type(k).__name__}\")\n",
        "            # bool — подкласс int, но это почти всегда ошибка при логировании метрик\n",
        "            if isinstance(v, bool) or not isinstance(v, Real):\n",
        "                raise TypeError(f\"Значение для '{k}' должно быть числом (int/float), получено {type(v).__name__}\")\n",
        "\n",
        "        gstep = self._step if step is None else int(step)\n",
        "\n",
        "        for k, v in row.items():\n",
        "            tag = f\"{self.tag_prefix}{k}\"\n",
        "            self._writer.add_scalar(tag, float(v), global_step=gstep)\n",
        "\n",
        "        if step is None:\n",
        "            self._step += 1\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        self._writer.flush()\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self._writer.close()\n",
        "\n",
        "    def __enter__(self) -> \"TBLogger\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb) -> None:\n",
        "        self.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4354e5b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# допустим, paths = generate_paths(...)\n",
        "tb_train = TBLogger(paths.tb_train)              # .../tb_logs/train\n",
        "tb_eval  = TBLogger(paths.tb_eval, tag_prefix=\"eval\")  # префикс опционален\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dc2ee442",
      "metadata": {},
      "outputs": [],
      "source": [
        "tb_train.log({\"loss\": 0.42, \"reward\": -1.5}, step=1)\n",
        "tb_train.log({\"loss\": 0.37, \"reward\": -0.9}, step=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b2e2dd72",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with TBLogger(paths.tb_eval) as tbe:\n",
        "    tbe.log({\"avg_return\": 10.3, \"success\": 0.62}, step=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "76b76342",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Mapping, MutableMapping, Optional, Union, Dict, Any, Sequence\n",
        "from numbers import Real\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    import torchrl  # noqa: F401\n",
        "    _TORCHRL_VER = getattr(torchrl, \"__version__\", \"unknown\")\n",
        "except Exception:\n",
        "    _TORCHRL_VER = None\n",
        "\n",
        "\n",
        "Stateful = Any  # Любой объект с методами state_dict()/load_state_dict()\n",
        "\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Универсальный менеджер чекпоинтов для PyTorch / TorchRL.\n",
        "\n",
        "    Сохраняет только state_dict'и (без сериализации самих объектов).\n",
        "    Имя файлов:\n",
        "      - last.pt — последний сохранённый\n",
        "      - best.pt — лучший по выбранной метрике\n",
        "      - step_<n>.pt — снапшоты по шагам/итерациям\n",
        "\n",
        "    Пример:\n",
        "        mgr = CheckpointManager(\n",
        "            ckpt_dir=paths.ckpt_dir,\n",
        "            statefuls={\"agent\": agent, \"optim\": optim, \"sched\": sched, \"buffer\": replay},\n",
        "            meta={\"algo\": \"PPO\", \"env\": \"MPE_Spread\", \"seed\": 42},\n",
        "            best_metric_key=\"eval/return_mean\",\n",
        "            mode=\"max\",\n",
        "            max_to_keep=5,\n",
        "        )\n",
        "        mgr.save(step=1000, metrics={\"eval/return_mean\": 12.3})\n",
        "        mgr.save(step=2000, metrics={\"eval/return_mean\": 15.0})  # обновит best.pt\n",
        "\n",
        "        # Возврат к лучшему\n",
        "        mgr.load(\"best\", strict=False, map_location=\"cpu\")\n",
        "\n",
        "    Параметры:\n",
        "      - ckpt_dir: каталог для чекпоинтов\n",
        "      - statefuls: объекты с .state_dict() / .load_state_dict(), например:\n",
        "          {\"policy\": model, \"optim\": optimizer, \"sched\": scheduler, \"buffer\": replay}\n",
        "      - meta: любые доп. поля (алгоритм, среда, сид и т.п.) — попадут в файл\n",
        "      - best_metric_key: имя метрики для выбора лучшего чекпоинта\n",
        "      - mode: \"min\" или \"max\" (как интерпретировать лучшую метрику)\n",
        "      - max_to_keep: максимум файлов step_<n>.pt (старые будут удаляться)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ckpt_dir: Union[str, Path],\n",
        "        statefuls: Optional[Mapping[str, Stateful]] = None,\n",
        "        *,\n",
        "        meta: Optional[Mapping[str, Any]] = None,\n",
        "        best_metric_key: Optional[str] = None,\n",
        "        mode: str = \"max\",\n",
        "        max_to_keep: int = 5,\n",
        "        map_location: Optional[Union[str, torch.device]] = None,\n",
        "    ) -> None:\n",
        "        self.ckpt_dir = Path(ckpt_dir)\n",
        "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.statefuls: Dict[str, Stateful] = dict(statefuls or {})\n",
        "        self.meta: Dict[str, Any] = dict(meta or {})\n",
        "        self.best_metric_key = best_metric_key\n",
        "        self.mode = mode.lower()\n",
        "        assert self.mode in {\"min\", \"max\"}, \"mode должен быть 'min' или 'max'\"\n",
        "        self.max_to_keep = int(max_to_keep)\n",
        "        self.map_location = map_location\n",
        "\n",
        "        # Текущая лучшая метрика (для быстрой проверки улучшения)\n",
        "        self._best_value: Optional[float] = None\n",
        "        # Попробуем прочитать из существующего best.pt\n",
        "        best_path = self.ckpt_dir / \"best.pt\"\n",
        "        if best_path.exists():\n",
        "            try:\n",
        "                payload = torch.load(best_path, map_location=\"cpu\")\n",
        "                self._best_value = self._extract_metric(payload)\n",
        "            except Exception:\n",
        "                warnings.warn(\"Не удалось прочитать существующий best.pt — пропускаю инициализацию лучшего значения.\")\n",
        "\n",
        "    # ---------- Публичные API ----------\n",
        "\n",
        "    def register(self, name: str, obj: Stateful) -> None:\n",
        "        \"\"\"Дорегистрировать объект по имени.\"\"\"\n",
        "        if name in self.statefuls:\n",
        "            warnings.warn(f\"Объект '{name}' уже зарегистрирован — будет перезаписан.\")\n",
        "        self.statefuls[name] = obj\n",
        "\n",
        "    def save(\n",
        "        self,\n",
        "        *,\n",
        "        step: int,\n",
        "        metrics: Optional[Mapping[str, Real]] = None,\n",
        "        make_step_snapshot: bool = True,\n",
        "        additional: Optional[Mapping[str, Any]] = None,\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Сохранить чекпоинт:\n",
        "          - last.pt всегда обновляется\n",
        "          - best.pt обновляется, если метрика улучшилась (при заданном best_metric_key)\n",
        "          - step_<n>.pt создаётся, если make_step_snapshot=True\n",
        "        Возвращает путь к last.pt.\n",
        "        \"\"\"\n",
        "        payload = self._build_payload(step=step, metrics=metrics, additional=additional)\n",
        "\n",
        "        # last.pt\n",
        "        last_path = self.ckpt_dir / \"last.pt\"\n",
        "        self._atomic_save(payload, last_path)\n",
        "\n",
        "        # step_<n>.pt\n",
        "        if make_step_snapshot:\n",
        "            step_path = self.ckpt_dir / f\"step_{int(step)}.pt\"\n",
        "            self._atomic_save(payload, step_path)\n",
        "            self._prune_step_checkpoints()\n",
        "\n",
        "        # best.pt\n",
        "        if self.best_metric_key is not None:\n",
        "            current = self._extract_metric(payload)\n",
        "            if current is not None and self._is_improved(current):\n",
        "                best_path = self.ckpt_dir / \"best.pt\"\n",
        "                self._atomic_save(payload, best_path)\n",
        "                self._best_value = float(current)\n",
        "\n",
        "        return last_path\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        which: Union[str, Path] = \"last\",\n",
        "        *,\n",
        "        strict: bool = True,\n",
        "        map_location: Optional[Union[str, torch.device]] = None,\n",
        "        return_payload: bool = False,\n",
        "    ) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Загрузить чекпоинт в зарегистрированные объекты.\n",
        "\n",
        "        which: \"last\" | \"best\" | Path к конкретному файлу.\n",
        "        strict: передаётся в load_state_dict для всех объектов.\n",
        "        map_location: переопределяет self.map_location.\n",
        "        return_payload: вернуть dict содержимого файла (по необходимости).\n",
        "        \"\"\"\n",
        "        path = self._resolve_path(which)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Файл чекпоинта не найден: {path}\")\n",
        "\n",
        "        ml = self.map_location if map_location is None else map_location\n",
        "        payload: Dict[str, Any] = torch.load(path, map_location=ml)\n",
        "\n",
        "        # Загружаем в объекты\n",
        "        states: Mapping[str, Any] = payload.get(\"statefuls\", {})\n",
        "        for name, obj in self.statefuls.items():\n",
        "            if name not in states:\n",
        "                warnings.warn(f\"В чекпоинте нет состояния для '{name}' — пропускаю.\")\n",
        "                continue\n",
        "            sd = states[name]\n",
        "            self._load_into(obj, sd, strict)\n",
        "\n",
        "        # Обновим локальный best (если подгрузили best.pt)\n",
        "        if (self.ckpt_dir / \"best.pt\") == Path(path):\n",
        "            self._best_value = self._extract_metric(payload)\n",
        "\n",
        "        return payload if return_payload else None\n",
        "\n",
        "    # ---------- Внутреннее ----------\n",
        "\n",
        "    def _build_payload(\n",
        "        self,\n",
        "        *,\n",
        "        step: int,\n",
        "        metrics: Optional[Mapping[str, Real]],\n",
        "        additional: Optional[Mapping[str, Any]],\n",
        "    ) -> Dict[str, Any]:\n",
        "        # Состояния всех зарегистрированных объектов\n",
        "        states: Dict[str, Any] = {}\n",
        "        for name, obj in self.statefuls.items():\n",
        "            if not hasattr(obj, \"state_dict\"):\n",
        "                raise TypeError(f\"Объект '{name}' не имеет метода state_dict()\")\n",
        "            states[name] = obj.state_dict()\n",
        "\n",
        "        # Метрики -> float\n",
        "        metrics_f: Dict[str, float] = {}\n",
        "        if metrics:\n",
        "            for k, v in metrics.items():\n",
        "                if isinstance(v, bool) or not isinstance(v, Real):\n",
        "                    raise TypeError(f\"Значение метрики '{k}' должно быть числом, получено {type(v).__name__}\")\n",
        "                metrics_f[k] = float(v)\n",
        "\n",
        "        payload: Dict[str, Any] = {\n",
        "            \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
        "            \"step\": int(step),\n",
        "            \"statefuls\": states,\n",
        "            \"metrics\": metrics_f,\n",
        "            \"meta\": {\n",
        "                **self.meta,\n",
        "                \"torch_version\": torch.__version__,\n",
        "                \"torchrl_version\": _TORCHRL_VER,\n",
        "            },\n",
        "        }\n",
        "        if additional:\n",
        "            payload[\"additional\"] = dict(additional)\n",
        "        return payload\n",
        "\n",
        "    def _atomic_save(self, payload: Dict[str, Any], path: Path) -> None:\n",
        "        tmp = path.with_suffix(path.suffix + \".tmp\")\n",
        "        torch.save(payload, tmp)\n",
        "        os.replace(tmp, path)\n",
        "\n",
        "    def _resolve_path(self, which: Union[str, Path]) -> Path:\n",
        "        if isinstance(which, Path):\n",
        "            return which\n",
        "        which = which.lower()\n",
        "        if which == \"last\":\n",
        "            return self.ckpt_dir / \"last.pt\"\n",
        "        if which == \"best\":\n",
        "            return self.ckpt_dir / \"best.pt\"\n",
        "        # иначе — это путь в виде строки\n",
        "        return Path(which)\n",
        "\n",
        "    def _is_improved(self, current: float) -> bool:\n",
        "        if self._best_value is None:\n",
        "            return True\n",
        "        if self.mode == \"max\":\n",
        "            return current > self._best_value\n",
        "        else:\n",
        "            return current < self._best_value\n",
        "\n",
        "    def _extract_metric(self, payload: Mapping[str, Any]) -> Optional[float]:\n",
        "        if self.best_metric_key is None:\n",
        "            return None\n",
        "        metrics = payload.get(\"metrics\", {})\n",
        "        val = metrics.get(self.best_metric_key, None)\n",
        "        try:\n",
        "            return float(val) if val is not None else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _load_into(self, obj: Stateful, state_dict: Mapping[str, Any], strict: bool) -> None:\n",
        "        if hasattr(obj, \"load_state_dict\"):\n",
        "            missing, unexpected = obj.load_state_dict(state_dict, strict=strict), ([], [])\n",
        "            # Некоторые реализации возвращают None — нормальная ситуация\n",
        "            if isinstance(missing, tuple) and len(missing) == 2:\n",
        "                missing, unexpected = missing\n",
        "            if (missing or unexpected) and not strict:\n",
        "                warnings.warn(f\"load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
        "        else:\n",
        "            raise TypeError(f\"Объект {obj!r} не поддерживает load_state_dict()\")\n",
        "\n",
        "    def _prune_step_checkpoints(self) -> None:\n",
        "        if self.max_to_keep is None or self.max_to_keep <= 0:\n",
        "            return\n",
        "        # Собираем step_*.pt и сортируем по номеру шага по убыванию\n",
        "        files = [p for p in self.ckpt_dir.glob(\"step_*.pt\") if p.is_file()]\n",
        "        def _step_num(p: Path) -> int:\n",
        "            stem = p.stem  # 'step_123'\n",
        "            if stem.startswith(\"step_\"):\n",
        "                tail = stem.split(\"_\", 1)[1]\n",
        "                return int(tail) if tail.isdigit() else -1\n",
        "            return -1\n",
        "        files.sort(key=_step_num, reverse=True)\n",
        "        # Удаляем старые сверх лимита\n",
        "        for p in files[self.max_to_keep:]:\n",
        "            try:\n",
        "                p.unlink(missing_ok=True)\n",
        "            except Exception:\n",
        "                warnings.warn(f\"Не удалось удалить старый чекпоинт: {p}\")\n",
        "\n",
        "    # ---------- Утилиты ----------\n",
        "\n",
        "    @property\n",
        "    def best_value(self) -> Optional[float]:\n",
        "        \"\"\"Текущее лучшее значение метрики (если задан best_metric_key).\"\"\"\n",
        "        return self._best_value\n",
        "\n",
        "    def list_checkpoints(self) -> Dict[str, Path]:\n",
        "        \"\"\"Быстрый список файлов чекпоинтов.\"\"\"\n",
        "        return {\n",
        "            \"last\": self.ckpt_dir / \"last.pt\",\n",
        "            \"best\": self.ckpt_dir / \"best.pt\",\n",
        "            \"steps\": [p for p in sorted(self.ckpt_dir.glob(\"step_*.pt\"))],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47ea80e",
      "metadata": {},
      "source": [
        "# Make Dirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "943cf5b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === infra.py (можно держать в той же ячейке, а потом вынести по модулям) ===\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import torch\n",
        "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# ---- Конфиги ----\n",
        "\n",
        "@dataclass\n",
        "class RunConfig:\n",
        "    root: Path\n",
        "    algo_name: str\n",
        "    env_name: str\n",
        "    run_time_fmt: str = \"%Y%m%d-%H%M%S\"\n",
        "    eval_every_batches: int = 5\n",
        "    eval_episodes: int = 5\n",
        "    keep_last_k_ckpts: int = 3\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    num_epochs: int = 10\n",
        "    sub_batch_size: int = 64\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "@dataclass\n",
        "class RunPaths:\n",
        "    root: Path\n",
        "    runs_dir: Path\n",
        "    run_dir: Path\n",
        "    csv_train_dir: Path\n",
        "    csv_eval_dir: Path\n",
        "    tb_train_dir: Path\n",
        "    tb_eval_dir: Path\n",
        "    txt_train_dir: Path\n",
        "    txt_eval_dir: Path\n",
        "    ckpt_dir: Path\n",
        "    meta_yaml: Path\n",
        "\n",
        "@dataclass\n",
        "class LoggerHandles:\n",
        "    train_csv: CSVLogger\n",
        "    eval_csv: CSVLogger\n",
        "    train_tb: TensorBoardLogger\n",
        "    eval_tb: TensorBoardLogger\n",
        "    train_txt_logger: logging.Logger\n",
        "    eval_txt_logger: logging.Logger\n",
        "\n",
        "# ---- FS helpers ----\n",
        "\n",
        "def _mkdir(p: Path) -> Path:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _make_run_dirs(cfg: RunConfig) -> RunPaths:\n",
        "    runs_dir = _mkdir(cfg.root / \"runs\")\n",
        "    run_name = f\"{cfg.algo_name}_{cfg.env_name}_{datetime.now().strftime(cfg.run_time_fmt)}\"\n",
        "    run_dir = _mkdir(runs_dir / run_name)\n",
        "\n",
        "    csv_train_dir = _mkdir(run_dir / \"csv_logs\" / \"train\")\n",
        "    csv_eval_dir  = _mkdir(run_dir / \"csv_logs\" / \"eval\")\n",
        "    tb_train_dir  = _mkdir(run_dir / \"tb_logs\"  / \"train\")\n",
        "    tb_eval_dir   = _mkdir(run_dir / \"tb_logs\"  / \"eval\")\n",
        "    txt_train_dir = _mkdir(run_dir / \"txt_logs\" / \"train\")\n",
        "    txt_eval_dir  = _mkdir(run_dir / \"txt_logs\" / \"eval\")\n",
        "    ckpt_dir      = _mkdir(run_dir / \"checkpoints\")\n",
        "    meta_yaml     = run_dir / \"meta.yaml\"\n",
        "\n",
        "    return RunPaths(\n",
        "        root=cfg.root, runs_dir=runs_dir, run_dir=run_dir,\n",
        "        csv_train_dir=csv_train_dir, csv_eval_dir=csv_eval_dir,\n",
        "        tb_train_dir=tb_train_dir, tb_eval_dir=tb_eval_dir,\n",
        "        txt_train_dir=txt_train_dir, txt_eval_dir=txt_eval_dir,\n",
        "        ckpt_dir=ckpt_dir, meta_yaml=meta_yaml\n",
        "    )\n",
        "\n",
        "def _safe_logger(name: str, file_path: Path) -> logging.Logger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.propagate = False\n",
        "    # чтобы не дублировать хендлеры при повторных запусках ячейки\n",
        "    if not any(isinstance(h, logging.FileHandler) and getattr(h, \"_file_path\", None) == str(file_path) \n",
        "               for h in logger.handlers):\n",
        "        fh = logging.FileHandler(file_path, encoding=\"utf-8\")\n",
        "        fh._file_path = str(file_path)\n",
        "        fmt = logging.Formatter(\"%(asctime)s | %(message)s\")\n",
        "        fh.setFormatter(fmt)\n",
        "        logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "def _get_loggers(paths: RunPaths) -> LoggerHandles:\n",
        "    # CSV/TB логгеры (раздельно train/eval)\n",
        "    train_csv = CSVLogger(save_dir=str(paths.csv_train_dir), name=\"metrics\")\n",
        "    eval_csv  = CSVLogger(save_dir=str(paths.csv_eval_dir),  name=\"metrics\")\n",
        "\n",
        "    train_tb  = TensorBoardLogger(save_dir=str(paths.tb_train_dir), name=\"tb\")\n",
        "    eval_tb   = TensorBoardLogger(save_dir=str(paths.tb_eval_dir),  name=\"tb\")\n",
        "\n",
        "    # Текстовые логи\n",
        "    train_txt = _safe_logger(f\"train_txt_{paths.run_dir.name}\", paths.txt_train_dir / \"train.log\")\n",
        "    eval_txt  = _safe_logger(f\"eval_txt_{paths.run_dir.name}\",  paths.txt_eval_dir / \"eval.log\")\n",
        "\n",
        "    return LoggerHandles(\n",
        "        train_csv=train_csv, eval_csv=eval_csv,\n",
        "        train_tb=train_tb,   eval_tb=eval_tb,\n",
        "        train_txt_logger=train_txt, eval_txt_logger=eval_txt\n",
        "    )\n",
        "\n",
        "def _dump_meta_yaml(paths: RunPaths, run_cfg: RunConfig, train_cfg: TrainConfig, extra: Optional[Dict[str, Any]] = None):\n",
        "    meta = {\n",
        "        \"run_name\": paths.run_dir.name,\n",
        "        \"algo\": run_cfg.algo_name,\n",
        "        \"env\": run_cfg.env_name,\n",
        "        \"started_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"train_config\": asdict(train_cfg),\n",
        "    }\n",
        "    if extra:\n",
        "        meta[\"extra\"] = extra\n",
        "    with open(paths.meta_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(meta, f, allow_unicode=True, sort_keys=False)\n",
        "\n",
        "# ---- Прогресс-бар ----\n",
        "\n",
        "def pbar_create(total_frames: int) -> tqdm:\n",
        "    return tqdm(total=total_frames, desc=\"initializing...\", leave=True, dynamic_ncols=True)\n",
        "\n",
        "def pbar_update(pbar: tqdm, batch_frames: int, desc_parts: Dict[str, Any]):\n",
        "    # аккуратное обновление, чтобы не переполнить total\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "    # человекочитаемое описание\n",
        "    formatted = []\n",
        "    for k, v in desc_parts.items():\n",
        "        if isinstance(v, float):\n",
        "            formatted.append(f\"{k}={v: .4f}\")\n",
        "        else:\n",
        "            formatted.append(f\"{k}: {v}\")\n",
        "    pbar.set_description(\", \".join(formatted))\n",
        "\n",
        "def pbar_write(pbar: tqdm, text: str):\n",
        "    pbar.write(text)\n",
        "\n",
        "def pbar_close(pbar: tqdm):\n",
        "    pbar.close()\n",
        "\n",
        "# ---- Логирование ----\n",
        "\n",
        "def _current_lr(optim: torch.optim.Optimizer, scheduler=None) -> float:\n",
        "    if scheduler is not None:\n",
        "        try:\n",
        "            return float(scheduler.get_last_lr()[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # берем lr первой группы\n",
        "    return float(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "def log_train_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    # CSV/TensorBoard\n",
        "    logs.train_csv.log_metrics(metrics, step=step)\n",
        "    logs.train_tb.log_metrics(metrics, step=step)\n",
        "    # TXT\n",
        "    logs.train_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "def log_eval_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    logs.eval_csv.log_metrics(metrics, step=step)\n",
        "    logs.eval_tb.log_metrics(metrics, step=step)\n",
        "    logs.eval_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "# ---- Чекпоинты ----\n",
        "\n",
        "def save_checkpoint(\n",
        "    paths: RunPaths,\n",
        "    *,\n",
        "    actor: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[Any],\n",
        "    step: int,\n",
        "    eval_metrics: Dict[str, Any],\n",
        "    algo_name: str,\n",
        "    env_name: str,\n",
        "    keep_last_k: int = 3\n",
        ") -> Path:\n",
        "    avg_ret = eval_metrics.get(\"return_mean\", None)\n",
        "    tag = f\"step{step}\"\n",
        "    if avg_ret is not None:\n",
        "        # безопасное имя файла\n",
        "        safe_ret = re.sub(r\"[^0-9eE\\-\\.+]\", \"\", f\"{avg_ret:.3f}\")\n",
        "        tag += f\"_ret{safe_ret}\"\n",
        "    ckpt_path = paths.ckpt_dir / f\"{algo_name}_{env_name}_{tag}.pt\"\n",
        "\n",
        "    payload = {\n",
        "        \"step\": step,\n",
        "        \"algo\": algo_name,\n",
        "        \"env\": env_name,\n",
        "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"model_state_dict\": actor.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"run_dir\": str(paths.run_dir),\n",
        "    }\n",
        "    torch.save(payload, ckpt_path)\n",
        "\n",
        "    # Retention policy: оставить только N последних\n",
        "    ckpts = sorted(paths.ckpt_dir.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if len(ckpts) > keep_last_k:\n",
        "        for old in ckpts[:-keep_last_k]:\n",
        "            try:\n",
        "                old.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return ckpt_path\n",
        "\n",
        "# ---- Инициализация всего набора ----\n",
        "\n",
        "def setup_run(\n",
        "    root: Path, algo_name: str, env_name: str,\n",
        "    train_cfg: TrainConfig,\n",
        "    run_cfg: Optional[RunConfig] = None,\n",
        "    meta_extra: Optional[Dict[str, Any]] = None\n",
        ") -> Tuple[RunPaths, LoggerHandles, RunConfig]:\n",
        "    run_cfg = run_cfg or RunConfig(root=root, algo_name=algo_name, env_name=env_name)\n",
        "    paths = _make_run_dirs(run_cfg)\n",
        "    _dump_meta_yaml(paths, run_cfg, train_cfg, extra=meta_extra)\n",
        "    loggers = _get_loggers(paths)\n",
        "    return paths, loggers, run_cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfa17c6",
      "metadata": {},
      "source": [
        "\n",
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7ae8fc8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def _safe_logger(name: str, file_path: Path) -> logging.Logger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.propagate = False\n",
        "    # чтобы не дублировать хендлеры при повторных запусках ячейки\n",
        "    if not any(isinstance(h, logging.FileHandler) and getattr(h, \"_file_path\", None) == str(file_path) \n",
        "               for h in logger.handlers):\n",
        "        fh = logging.FileHandler(file_path, encoding=\"utf-8\")\n",
        "        fh._file_path = str(file_path)\n",
        "        fmt = logging.Formatter(\"%(asctime)s | %(message)s\")\n",
        "        fh.setFormatter(fmt)\n",
        "        logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "def _get_loggers(paths: RunPaths) -> LoggerHandles:\n",
        "    # CSV/TB логгеры (раздельно train/eval)\n",
        "    train_csv = CSVLogger(save_dir=str(paths.csv_train_dir), name=\"metrics\")\n",
        "    eval_csv  = CSVLogger(save_dir=str(paths.csv_eval_dir),  name=\"metrics\")\n",
        "\n",
        "    train_tb  = TensorBoardLogger(save_dir=str(paths.tb_train_dir), name=\"tb\")\n",
        "    eval_tb   = TensorBoardLogger(save_dir=str(paths.tb_eval_dir),  name=\"tb\")\n",
        "\n",
        "    # Текстовые логи\n",
        "    train_txt = _safe_logger(f\"train_txt_{paths.run_dir.name}\", paths.txt_train_dir / \"train.log\")\n",
        "    eval_txt  = _safe_logger(f\"eval_txt_{paths.run_dir.name}\",  paths.txt_eval_dir / \"eval.log\")\n",
        "\n",
        "    return LoggerHandles(\n",
        "        train_csv=train_csv, eval_csv=eval_csv,\n",
        "        train_tb=train_tb,   eval_tb=eval_tb,\n",
        "        train_txt_logger=train_txt, eval_txt_logger=eval_txt\n",
        "    )\n",
        "\n",
        "def _dump_meta_yaml(paths: RunPaths, run_cfg: RunConfig, train_cfg: TrainConfig, extra: Optional[Dict[str, Any]] = None):\n",
        "    meta = {\n",
        "        \"run_name\": paths.run_dir.name,\n",
        "        \"algo\": run_cfg.algo_name,\n",
        "        \"env\": run_cfg.env_name,\n",
        "        \"started_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"train_config\": asdict(train_cfg),\n",
        "    }\n",
        "    if extra:\n",
        "        meta[\"extra\"] = extra\n",
        "    with open(paths.meta_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(meta, f, allow_unicode=True, sort_keys=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39743834",
      "metadata": {},
      "source": [
        "# Progress Bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "758e6025",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def pbar_create(total_frames: int) -> tqdm:\n",
        "    return tqdm(total=total_frames, desc=\"initializing...\", leave=True, dynamic_ncols=True)\n",
        "\n",
        "def pbar_update(pbar: tqdm, batch_frames: int, desc_parts: Dict[str, Any]):\n",
        "    # аккуратное обновление, чтобы не переполнить total\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "    # человекочитаемое описание\n",
        "    formatted = []\n",
        "    for k, v in desc_parts.items():\n",
        "        if isinstance(v, float):\n",
        "            formatted.append(f\"{k}={v: .4f}\")\n",
        "        else:\n",
        "            formatted.append(f\"{k}: {v}\")\n",
        "    pbar.set_description(\", \".join(formatted))\n",
        "\n",
        "def pbar_write(pbar: tqdm, text: str):\n",
        "    pbar.write(text)\n",
        "\n",
        "def pbar_close(pbar: tqdm):\n",
        "    pbar.close()\n",
        "\n",
        "# ---- Логирование ----\n",
        "\n",
        "def _current_lr(optim: torch.optim.Optimizer, scheduler=None) -> float:\n",
        "    if scheduler is not None:\n",
        "        try:\n",
        "            return float(scheduler.get_last_lr()[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # берем lr первой группы\n",
        "    return float(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "def log_train_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    # CSV/TensorBoard\n",
        "    logs.train_csv.log_metrics(metrics, step=step)\n",
        "    logs.train_tb.log_metrics(metrics, step=step)\n",
        "    # TXT\n",
        "    logs.train_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "def log_eval_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    logs.eval_csv.log_metrics(metrics, step=step)\n",
        "    logs.eval_tb.log_metrics(metrics, step=step)\n",
        "    logs.eval_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b67a6e",
      "metadata": {},
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b3e46dee",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(\n",
        "    paths: RunPaths,\n",
        "    *,\n",
        "    actor: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[Any],\n",
        "    step: int,\n",
        "    eval_metrics: Dict[str, Any],\n",
        "    algo_name: str,\n",
        "    env_name: str,\n",
        "    keep_last_k: int = 3\n",
        ") -> Path:\n",
        "    avg_ret = eval_metrics.get(\"return_mean\", None)\n",
        "    tag = f\"step{step}\"\n",
        "    if avg_ret is not None:\n",
        "        # безопасное имя файла\n",
        "        safe_ret = re.sub(r\"[^0-9eE\\-\\.+]\", \"\", f\"{avg_ret:.3f}\")\n",
        "        tag += f\"_ret{safe_ret}\"\n",
        "    ckpt_path = paths.ckpt_dir / f\"{algo_name}_{env_name}_{tag}.pt\"\n",
        "\n",
        "    payload = {\n",
        "        \"step\": step,\n",
        "        \"algo\": algo_name,\n",
        "        \"env\": env_name,\n",
        "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"model_state_dict\": actor.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"run_dir\": str(paths.run_dir),\n",
        "    }\n",
        "    torch.save(payload, ckpt_path)\n",
        "\n",
        "    # Retention policy: оставить только N последних\n",
        "    ckpts = sorted(paths.ckpt_dir.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if len(ckpts) > keep_last_k:\n",
        "        for old in ckpts[:-keep_last_k]:\n",
        "            try:\n",
        "                old.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return ckpt_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c93040",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2fe88d15",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34225acf012c4a9e86eed1f29a8a8c35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PPO:   0%|          | 0.00/10.0k [00:00<?, ?frames/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval: avg reward = 73.22015075683593, max episode lengh = 8\n",
            "eval: avg reward = 67.83404083251953, max episode lengh = 8\n"
          ]
        }
      ],
      "source": [
        "from contextlib import contextmanager\n",
        "from tqdm.auto import tqdm\n",
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def progress_bar(total_frames: int, desc: str = \"train\"):\n",
        "    pbar = tqdm(\n",
        "        total=total_frames,\n",
        "        desc=desc,\n",
        "        dynamic_ncols=True,\n",
        "        leave=True,\n",
        "        unit=\"frames\",\n",
        "        unit_scale=True,\n",
        "        smoothing=0.1,\n",
        "    )\n",
        "    try:\n",
        "        yield pbar\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "# ---- в вашем цикле ----\n",
        "with progress_bar(total_frames, desc=\"PPO\") as pbar:\n",
        "    for i, tensordict_data in enumerate(collector):\n",
        "        # ... обучение ...\n",
        "        # логируем метрики\n",
        "        reward = tensordict_data[\"next\", \"reward\"].mean().item()\n",
        "        step_count = tensordict_data[\"step_count\"].max().item()\n",
        "        lr = optim.param_groups[0][\"lr\"]\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            eval_results = eval_policy(env, actor, episodes=5, progress=False)\n",
        "            pbar.write(\n",
        "                f'eval: avg reward = {eval_results[\"return_mean\"]}, '\n",
        "                f'max episode lengh = {eval_results[\"max_episode_lengh\"]}'\n",
        "            )\n",
        "\n",
        "        # корректное число фреймов в пачке\n",
        "        batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "        if batch_frames > 0:\n",
        "            remaining = pbar.total - pbar.n\n",
        "            pbar.update(min(batch_frames, remaining))\n",
        "\n",
        "        # метрики в postfix (дешевле и чище, чем set_description)\n",
        "        pbar.set_postfix(\n",
        "            {\"avg_reward\": f\"{reward: .4f}\",\n",
        "             \"max_step\": int(step_count),\n",
        "             \"lr\": f\"{lr: .4e}\"},\n",
        "            refresh=False\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875bff8b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10e58c55a2004b70905fd64ed493252a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "initializing...:   0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval: avg reward = 118.453, max episode length = 15\n",
            "eval: avg reward = 153.850, max episode length = 22\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# === training.py (пример использования) ===\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# Ваши заранее определённые объекты/переменные:\n",
        "# env, actor, collector, advantage_module, replay_buffer, loss_module, optim, scheduler, frames_per_batch, device\n",
        "# а также общее число фреймов для тренировки (например, total_frames)\n",
        "# здесь считаем, что total_frames известен извне (например, как параметр эксперимента)\n",
        "\n",
        "# Корневой каталог и названия алгоритма и среды\n",
        "from pathlib import Path\n",
        "ROOT = Path(\"..\").resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "\n",
        "# Конфиги\n",
        "train_cfg = TrainConfig(num_epochs=10, sub_batch_size=64, max_grad_norm=1.0)\n",
        "# eval каждые 5 батчей — совпадает с исходным кодом\n",
        "run_cfg   = RunConfig(root=ROOT, algo_name=ALGO_NAME, env_name=ENV_NAME, eval_every_batches=5, eval_episodes=5, keep_last_k_ckpts=3)\n",
        "\n",
        "# Инициализация run-директории, логгеров и метаинформации\n",
        "meta_extra = {\n",
        "    \"frames_per_batch\": int(frames_per_batch),\n",
        "    \"device\": str(device),\n",
        "}\n",
        "paths, logs, run_cfg = setup_run(ROOT, ALGO_NAME, ENV_NAME, train_cfg, run_cfg, meta_extra=meta_extra)\n",
        "\n",
        "# Подготовка прогресс-бара\n",
        "# total_frames — количество фреймов, которое вы планируете собрать/обучить за весь ран\n",
        "pbar = pbar_create(total_frames=total_frames)\n",
        "\n",
        "global_frames = 0  # будем логировать шаги в терминах фреймов (удобно для RL)\n",
        "batch_index = 0\n",
        "\n",
        "try:\n",
        "    for i, tensordict_data in enumerate(collector):\n",
        "        batch_index += 1\n",
        "\n",
        "        # === Обучение на партии ===\n",
        "        for _ in range(train_cfg.num_epochs):\n",
        "            # Advantage пересчитываем на каждом проходе\n",
        "            advantage_module(tensordict_data)\n",
        "            data_view = tensordict_data.reshape(-1)\n",
        "            replay_buffer.extend(data_view.cpu())\n",
        "            # Разбиваем на саббатчи\n",
        "            iters = int(frames_per_batch) // int(train_cfg.sub_batch_size)\n",
        "            for _ in range(iters):\n",
        "                subdata = replay_buffer.sample(train_cfg.sub_batch_size)\n",
        "                loss_vals = loss_module(subdata.to(device))\n",
        "                loss_total = (\n",
        "                    loss_vals[\"loss_objective\"] \n",
        "                    + loss_vals[\"loss_critic\"] \n",
        "                    + loss_vals[\"loss_entropy\"]\n",
        "                )\n",
        "\n",
        "                # Оптимизация\n",
        "                loss_total.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), train_cfg.max_grad_norm)\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "\n",
        "        # шаг планировщика\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # === Подсчёт метрик тренировки ===\n",
        "        # корректное число фреймов в текущей пачке\n",
        "        batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "        global_frames += batch_frames\n",
        "\n",
        "        # базовые метрики: средняя награда по партии, лоссы, lr\n",
        "        with torch.no_grad():\n",
        "            avg_reward = float(tensordict_data.get((\"next\", \"reward\")).float().mean().cpu().item())\n",
        "        lr_val = _current_lr(optim, scheduler=scheduler)\n",
        "\n",
        "        train_metrics = {\n",
        "            \"reward\": avg_reward,\n",
        "            \"loss_objective\": float(loss_vals[\"loss_objective\"].detach().cpu().item()),\n",
        "            \"loss_critic\": float(loss_vals[\"loss_critic\"].detach().cpu().item()),\n",
        "            \"loss_entropy\": float(loss_vals[\"loss_entropy\"].detach().cpu().item()),\n",
        "            \"loss_total\": float(loss_total.detach().cpu().item()),\n",
        "            \"lr\": lr_val,\n",
        "            \"batch_frames\": float(batch_frames),   # полезно иметь и в csv\n",
        "            \"global_frames\": float(global_frames)  # для графиков\n",
        "        }\n",
        "        log_train_metrics(logs, train_metrics, step=global_frames)\n",
        "\n",
        "        # === Обновление прогресс-бара ===\n",
        "        pbar_update(\n",
        "            pbar,\n",
        "            batch_frames=batch_frames,\n",
        "            desc_parts={\n",
        "                \"avg reward\": avg_reward,\n",
        "                \"frames\": global_frames,\n",
        "                \"lr\": lr_val\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # === Периодическая оценка ===\n",
        "        if (batch_index % run_cfg.eval_every_batches) == 0:\n",
        "            eval_results = eval_policy(env, actor, episodes=run_cfg.eval_episodes, progress=False)\n",
        "            # ожидаем, что eval_policy вернёт хотя бы 'return_mean' и 'max_episode_lengh'\n",
        "            eval_metrics = {\n",
        "                \"return_mean\": float(eval_results.get(\"return_mean\", float(\"nan\"))),\n",
        "                \"max_episode_length\": float(eval_results.get(\"max_episode_lengh\", float(\"nan\"))),\n",
        "                \"global_frames\": float(global_frames)\n",
        "            }\n",
        "            # логируем отдельно в eval-логи\n",
        "            log_eval_metrics(logs, eval_metrics, step=global_frames)\n",
        "\n",
        "            # пишем в прогресс-бар (не ломая его)\n",
        "            pbar_write(\n",
        "                pbar,\n",
        "                f\"eval: avg reward = {eval_metrics['return_mean']:.3f}, \"\n",
        "                f\"max episode length = {eval_metrics['max_episode_length']:.0f}\"\n",
        "            )\n",
        "\n",
        "            # сохраняем чекпоинт по результатам оценки\n",
        "            ckpt_path = save_checkpoint(\n",
        "                paths,\n",
        "                actor=actor,\n",
        "                optimizer=optim,\n",
        "                scheduler=scheduler,\n",
        "                step=global_frames,\n",
        "                eval_metrics=eval_metrics,\n",
        "                algo_name=ALGO_NAME,\n",
        "                env_name=ENV_NAME,\n",
        "                keep_last_k=run_cfg.keep_last_k_ckpts\n",
        "            )\n",
        "            logs.eval_txt_logger.info(f\"checkpoint saved: {ckpt_path.name}\")\n",
        "\n",
        "        # опционально — условие выхода, если собрали достаточно фреймов\n",
        "        if global_frames >= pbar.total:\n",
        "            break\n",
        "\n",
        "finally:\n",
        "    # Гарантированно закрываем бар (важно для корректного вывода в ноутбуках)\n",
        "    pbar_close(pbar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f68cfcc",
      "metadata": {},
      "source": [
        "# Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7abc3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using run_dir: C:\\Users\\werna\\Documents\\GitHub\\AgentsLab\\runs\\ppo_pendulum_20250822-192147\n",
            "Train columns: []\n",
            "Eval columns: []\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'step'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# === Примеры графиков ===\u001b[39;00m\n\u001b[32m     53\u001b[39m plt.figure()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m plt.plot(\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df_train.get(\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m, pd.Series([\u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m)]*\u001b[38;5;28mlen\u001b[39m(df_train))))\n\u001b[32m     55\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mTrain: average reward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mglobal frames (step)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\werna\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\werna\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'step'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# === plotting_example.py (запускать в новой ячейке после обучения) ===\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Укажите путь к последнему ран-каналу, либо найдите автоматически:\n",
        "ROOT = Path(\"..\").resolve()\n",
        "runs_dir = ROOT / \"runs\"\n",
        "\n",
        "# Найдём последний ран (по времени модификации папки)\n",
        "run_dirs = sorted([p for p in runs_dir.glob(\"*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "assert len(run_dirs) > 0, \"Не найдено ни одного запуска в runs/\"\n",
        "run_dir = run_dirs[0]\n",
        "print(\"Using run_dir:\", run_dir)\n",
        "\n",
        "def _load_pl_csv(csv_root: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    CSVLogger из Lightning создаёт иерархию: <csv_root>/<name>/version_x/metrics.csv\n",
        "    и может писать в «длинном» формате (name, step, value) либо «широком» (step + метрики).\n",
        "    Эта функция аккуратно приводить к широкому формату: столбцы — метрики, индекс — step.\n",
        "    \"\"\"\n",
        "    # ищем все варианты metrics.csv\n",
        "    metrics_files = list(csv_root.glob(\"**/metrics.csv\"))\n",
        "    if not metrics_files:\n",
        "        return pd.DataFrame()\n",
        "    # берём последний по времени\n",
        "    metrics_path = sorted(metrics_files, key=lambda p: p.stat().st_mtime)[-1]\n",
        "    df = pd.read_csv(metrics_path)\n",
        "\n",
        "    if {\"name\", \"step\", \"value\"}.issubset(df.columns):\n",
        "        # длинный формат -> pivot\n",
        "        wide = df.pivot_table(index=\"step\", columns=\"name\", values=\"value\", aggfunc=\"last\")\n",
        "        wide.sort_index(inplace=True)\n",
        "        wide.reset_index(inplace=True)\n",
        "        return wide\n",
        "    else:\n",
        "        # уже широкий формат\n",
        "        if \"step\" not in df.columns:\n",
        "            # если step отсутствует — добавим монотонный по индексу\n",
        "            df.insert(0, \"step\", range(len(df)))\n",
        "        return df\n",
        "\n",
        "train_csv_root = run_dir / \"csv_logs\" / \"train\"\n",
        "eval_csv_root  = run_dir / \"csv_logs\" / \"eval\"\n",
        "\n",
        "df_train = _load_pl_csv(train_csv_root)\n",
        "df_eval  = _load_pl_csv(eval_csv_root)\n",
        "\n",
        "print(\"Train columns:\", df_train.columns.tolist())\n",
        "print(\"Eval columns:\", df_eval.columns.tolist())\n",
        "\n",
        "# === Примеры графиков ===\n",
        "plt.figure()\n",
        "plt.plot(df_train[\"step\"], df_train.get(\"reward\", pd.Series([float(\"nan\")]*len(df_train))))\n",
        "plt.title(\"Train: average reward\")\n",
        "plt.xlabel(\"global frames (step)\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "if \"loss_total\" in df_train.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_train[\"step\"], df_train[\"loss_total\"])\n",
        "    plt.title(\"Train: total loss\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"loss_total\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if not df_eval.empty and \"return_mean\" in df_eval.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_eval[\"step\"], df_eval[\"return_mean\"], marker=\"o\")\n",
        "    plt.title(\"Eval: return_mean\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"return_mean\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea88a1de",
      "metadata": {},
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ade9f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91d7dbc95013412e9eda87a6b073c091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "eval:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'return_mean': 179.49685134887696,\n",
              " 'return_sum': 8974.842567443848,\n",
              " 'max_episode_lengh': 33,\n",
              " 'num_episodes': 50}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "eval_policy(env, actor, episodes=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "marl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
