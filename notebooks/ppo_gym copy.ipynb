{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7a86d02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Общее конфигурирование\n",
        "from agentslab.utils.device import resolve_device\n",
        "from agentslab.utils.seeding import set_global_seed\n",
        "from pathlib import Path\n",
        "\n",
        "device = resolve_device(\"cpu\")\n",
        "print('Device:', device)\n",
        " \n",
        "seed = 42\n",
        "set_global_seed(seed, deterministic=True)\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "ENV_ID = \"InvertedDoublePendulum-v4\"\n",
        "# ENV_ID = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54dd996",
      "metadata": {},
      "source": [
        "# Создание среды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d131b0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m2025-08-22 19:21:44,105 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from agentslab.envs.gym_factory import GymEnvConfig, make_gym_env\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "env_cfg = GymEnvConfig(env_id=ENV_ID, render_mode=None, device=device, seed=seed)\n",
        "env = make_gym_env(env_cfg)\n",
        "check_env_specs(env)\n",
        "\n",
        "# from agentslab.utils.specs import print_specs\n",
        "# print_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c427be",
      "metadata": {},
      "source": [
        "# Создание актора и критика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3824baf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ValueOperator(\n",
              "    module=Sequential(\n",
              "      (0): Linear(in_features=11, out_features=256, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Tanh()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    ),\n",
              "    device=cpu,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['state_value'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.modules.networks import MLPConfig, build_mlp\n",
        "from agentslab.modules.policy import build_stochastic_actor\n",
        "from torchrl.modules import ValueOperator\n",
        "\n",
        "# Достаём размерности\n",
        "obs_dim = env.observation_spec[\"observation\"].shape[-1]\n",
        "act_dim = env.action_spec.shape[-1]\n",
        "\n",
        "mlp_cfg = MLPConfig(\n",
        "        in_dim = obs_dim, \n",
        "        out_dim = 2*act_dim,\n",
        "        hidden_sizes = (256, 256),\n",
        "        activation = \"tanh\",\n",
        "        layer_norm = False\n",
        ")\n",
        "\n",
        "actor_network = build_mlp(mlp_cfg)\n",
        "actor = build_stochastic_actor(actor_network, env.action_spec)\n",
        "\n",
        "mlp_cfg.out_dim = act_dim\n",
        "critic_network = build_mlp(mlp_cfg)\n",
        "critic = ValueOperator(module=critic_network, in_keys=[\"observation\"])\n",
        "critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebea77ec",
      "metadata": {},
      "source": [
        "# Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9cdc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "frames_per_batch = 1000\n",
        "# For a complete training, bring the number of frames up to 1M\n",
        "total_frames = 10_000\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    create_env_fn=env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    split_trajs=False,\n",
        "    device=device,\n",
        ")\n",
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8859fad",
      "metadata": {},
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e45ce2a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "import torch\n",
        "\n",
        "gamma = 0.99\n",
        "lmbda = 0.95\n",
        "\n",
        "advantage_module = GAE(\n",
        "    gamma=gamma, lmbda=lmbda, value_network=critic, average_gae=True\n",
        ")\n",
        "\n",
        "clip_epsilon = (\n",
        "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
        ")\n",
        "entropy_eps = 1e-4\n",
        "\n",
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=clip_epsilon,\n",
        "    entropy_bonus=bool(entropy_eps),\n",
        "    entropy_coeff=entropy_eps,\n",
        "    # these keys match by default but we set this for completeness\n",
        "    critic_coeff=1.0,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        ")\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optim, \n",
        "    T_max=total_frames // frames_per_batch, \n",
        "    eta_min=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f0b4e9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47ea80e",
      "metadata": {},
      "source": [
        "# Make Dirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "943cf5b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === infra.py (можно держать в той же ячейке, а потом вынести по модулям) ===\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import torch\n",
        "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# ---- Конфиги ----\n",
        "\n",
        "@dataclass\n",
        "class RunConfig:\n",
        "    root: Path\n",
        "    algo_name: str\n",
        "    env_name: str\n",
        "    run_time_fmt: str = \"%Y%m%d-%H%M%S\"\n",
        "    eval_every_batches: int = 5\n",
        "    eval_episodes: int = 5\n",
        "    keep_last_k_ckpts: int = 3\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    num_epochs: int = 10\n",
        "    sub_batch_size: int = 64\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "@dataclass\n",
        "class RunPaths:\n",
        "    root: Path\n",
        "    runs_dir: Path\n",
        "    run_dir: Path\n",
        "    csv_train_dir: Path\n",
        "    csv_eval_dir: Path\n",
        "    tb_train_dir: Path\n",
        "    tb_eval_dir: Path\n",
        "    txt_train_dir: Path\n",
        "    txt_eval_dir: Path\n",
        "    ckpt_dir: Path\n",
        "    meta_yaml: Path\n",
        "\n",
        "@dataclass\n",
        "class LoggerHandles:\n",
        "    train_csv: CSVLogger\n",
        "    eval_csv: CSVLogger\n",
        "    train_tb: TensorBoardLogger\n",
        "    eval_tb: TensorBoardLogger\n",
        "    train_txt_logger: logging.Logger\n",
        "    eval_txt_logger: logging.Logger\n",
        "\n",
        "# ---- FS helpers ----\n",
        "\n",
        "def _mkdir(p: Path) -> Path:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _make_run_dirs(cfg: RunConfig) -> RunPaths:\n",
        "    runs_dir = _mkdir(cfg.root / \"runs\")\n",
        "    run_name = f\"{cfg.algo_name}_{cfg.env_name}_{datetime.now().strftime(cfg.run_time_fmt)}\"\n",
        "    run_dir = _mkdir(runs_dir / run_name)\n",
        "\n",
        "    csv_train_dir = _mkdir(run_dir / \"csv_logs\" / \"train\")\n",
        "    csv_eval_dir  = _mkdir(run_dir / \"csv_logs\" / \"eval\")\n",
        "    tb_train_dir  = _mkdir(run_dir / \"tb_logs\"  / \"train\")\n",
        "    tb_eval_dir   = _mkdir(run_dir / \"tb_logs\"  / \"eval\")\n",
        "    txt_train_dir = _mkdir(run_dir / \"txt_logs\" / \"train\")\n",
        "    txt_eval_dir  = _mkdir(run_dir / \"txt_logs\" / \"eval\")\n",
        "    ckpt_dir      = _mkdir(run_dir / \"checkpoints\")\n",
        "    meta_yaml     = run_dir / \"meta.yaml\"\n",
        "\n",
        "    return RunPaths(\n",
        "        root=cfg.root, runs_dir=runs_dir, run_dir=run_dir,\n",
        "        csv_train_dir=csv_train_dir, csv_eval_dir=csv_eval_dir,\n",
        "        tb_train_dir=tb_train_dir, tb_eval_dir=tb_eval_dir,\n",
        "        txt_train_dir=txt_train_dir, txt_eval_dir=txt_eval_dir,\n",
        "        ckpt_dir=ckpt_dir, meta_yaml=meta_yaml\n",
        "    )\n",
        "\n",
        "def _safe_logger(name: str, file_path: Path) -> logging.Logger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.propagate = False\n",
        "    # чтобы не дублировать хендлеры при повторных запусках ячейки\n",
        "    if not any(isinstance(h, logging.FileHandler) and getattr(h, \"_file_path\", None) == str(file_path) \n",
        "               for h in logger.handlers):\n",
        "        fh = logging.FileHandler(file_path, encoding=\"utf-8\")\n",
        "        fh._file_path = str(file_path)\n",
        "        fmt = logging.Formatter(\"%(asctime)s | %(message)s\")\n",
        "        fh.setFormatter(fmt)\n",
        "        logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "def _get_loggers(paths: RunPaths) -> LoggerHandles:\n",
        "    # CSV/TB логгеры (раздельно train/eval)\n",
        "    train_csv = CSVLogger(save_dir=str(paths.csv_train_dir), name=\"metrics\")\n",
        "    eval_csv  = CSVLogger(save_dir=str(paths.csv_eval_dir),  name=\"metrics\")\n",
        "\n",
        "    train_tb  = TensorBoardLogger(save_dir=str(paths.tb_train_dir), name=\"tb\")\n",
        "    eval_tb   = TensorBoardLogger(save_dir=str(paths.tb_eval_dir),  name=\"tb\")\n",
        "\n",
        "    # Текстовые логи\n",
        "    train_txt = _safe_logger(f\"train_txt_{paths.run_dir.name}\", paths.txt_train_dir / \"train.log\")\n",
        "    eval_txt  = _safe_logger(f\"eval_txt_{paths.run_dir.name}\",  paths.txt_eval_dir / \"eval.log\")\n",
        "\n",
        "    return LoggerHandles(\n",
        "        train_csv=train_csv, eval_csv=eval_csv,\n",
        "        train_tb=train_tb,   eval_tb=eval_tb,\n",
        "        train_txt_logger=train_txt, eval_txt_logger=eval_txt\n",
        "    )\n",
        "\n",
        "def _dump_meta_yaml(paths: RunPaths, run_cfg: RunConfig, train_cfg: TrainConfig, extra: Optional[Dict[str, Any]] = None):\n",
        "    meta = {\n",
        "        \"run_name\": paths.run_dir.name,\n",
        "        \"algo\": run_cfg.algo_name,\n",
        "        \"env\": run_cfg.env_name,\n",
        "        \"started_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"train_config\": asdict(train_cfg),\n",
        "    }\n",
        "    if extra:\n",
        "        meta[\"extra\"] = extra\n",
        "    with open(paths.meta_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(meta, f, allow_unicode=True, sort_keys=False)\n",
        "\n",
        "# ---- Прогресс-бар ----\n",
        "\n",
        "def pbar_create(total_frames: int) -> tqdm:\n",
        "    return tqdm(total=total_frames, desc=\"initializing...\", leave=True, dynamic_ncols=True)\n",
        "\n",
        "def pbar_update(pbar: tqdm, batch_frames: int, desc_parts: Dict[str, Any]):\n",
        "    # аккуратное обновление, чтобы не переполнить total\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "    # человекочитаемое описание\n",
        "    formatted = []\n",
        "    for k, v in desc_parts.items():\n",
        "        if isinstance(v, float):\n",
        "            formatted.append(f\"{k}={v: .4f}\")\n",
        "        else:\n",
        "            formatted.append(f\"{k}: {v}\")\n",
        "    pbar.set_description(\", \".join(formatted))\n",
        "\n",
        "def pbar_write(pbar: tqdm, text: str):\n",
        "    pbar.write(text)\n",
        "\n",
        "def pbar_close(pbar: tqdm):\n",
        "    pbar.close()\n",
        "\n",
        "# ---- Логирование ----\n",
        "\n",
        "def _current_lr(optim: torch.optim.Optimizer, scheduler=None) -> float:\n",
        "    if scheduler is not None:\n",
        "        try:\n",
        "            return float(scheduler.get_last_lr()[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # берем lr первой группы\n",
        "    return float(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "def log_train_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    # CSV/TensorBoard\n",
        "    logs.train_csv.log_metrics(metrics, step=step)\n",
        "    logs.train_tb.log_metrics(metrics, step=step)\n",
        "    # TXT\n",
        "    logs.train_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "def log_eval_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    logs.eval_csv.log_metrics(metrics, step=step)\n",
        "    logs.eval_tb.log_metrics(metrics, step=step)\n",
        "    logs.eval_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "# ---- Чекпоинты ----\n",
        "\n",
        "def save_checkpoint(\n",
        "    paths: RunPaths,\n",
        "    *,\n",
        "    actor: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[Any],\n",
        "    step: int,\n",
        "    eval_metrics: Dict[str, Any],\n",
        "    algo_name: str,\n",
        "    env_name: str,\n",
        "    keep_last_k: int = 3\n",
        ") -> Path:\n",
        "    avg_ret = eval_metrics.get(\"return_mean\", None)\n",
        "    tag = f\"step{step}\"\n",
        "    if avg_ret is not None:\n",
        "        # безопасное имя файла\n",
        "        safe_ret = re.sub(r\"[^0-9eE\\-\\.+]\", \"\", f\"{avg_ret:.3f}\")\n",
        "        tag += f\"_ret{safe_ret}\"\n",
        "    ckpt_path = paths.ckpt_dir / f\"{algo_name}_{env_name}_{tag}.pt\"\n",
        "\n",
        "    payload = {\n",
        "        \"step\": step,\n",
        "        \"algo\": algo_name,\n",
        "        \"env\": env_name,\n",
        "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"model_state_dict\": actor.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"run_dir\": str(paths.run_dir),\n",
        "    }\n",
        "    torch.save(payload, ckpt_path)\n",
        "\n",
        "    # Retention policy: оставить только N последних\n",
        "    ckpts = sorted(paths.ckpt_dir.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if len(ckpts) > keep_last_k:\n",
        "        for old in ckpts[:-keep_last_k]:\n",
        "            try:\n",
        "                old.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return ckpt_path\n",
        "\n",
        "# ---- Инициализация всего набора ----\n",
        "\n",
        "def setup_run(\n",
        "    root: Path, algo_name: str, env_name: str,\n",
        "    train_cfg: TrainConfig,\n",
        "    run_cfg: Optional[RunConfig] = None,\n",
        "    meta_extra: Optional[Dict[str, Any]] = None\n",
        ") -> Tuple[RunPaths, LoggerHandles, RunConfig]:\n",
        "    run_cfg = run_cfg or RunConfig(root=root, algo_name=algo_name, env_name=env_name)\n",
        "    paths = _make_run_dirs(run_cfg)\n",
        "    _dump_meta_yaml(paths, run_cfg, train_cfg, extra=meta_extra)\n",
        "    loggers = _get_loggers(paths)\n",
        "    return paths, loggers, run_cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfa17c6",
      "metadata": {},
      "source": [
        "\n",
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7ae8fc8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def _safe_logger(name: str, file_path: Path) -> logging.Logger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.propagate = False\n",
        "    # чтобы не дублировать хендлеры при повторных запусках ячейки\n",
        "    if not any(isinstance(h, logging.FileHandler) and getattr(h, \"_file_path\", None) == str(file_path) \n",
        "               for h in logger.handlers):\n",
        "        fh = logging.FileHandler(file_path, encoding=\"utf-8\")\n",
        "        fh._file_path = str(file_path)\n",
        "        fmt = logging.Formatter(\"%(asctime)s | %(message)s\")\n",
        "        fh.setFormatter(fmt)\n",
        "        logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "def _get_loggers(paths: RunPaths) -> LoggerHandles:\n",
        "    # CSV/TB логгеры (раздельно train/eval)\n",
        "    train_csv = CSVLogger(save_dir=str(paths.csv_train_dir), name=\"metrics\")\n",
        "    eval_csv  = CSVLogger(save_dir=str(paths.csv_eval_dir),  name=\"metrics\")\n",
        "\n",
        "    train_tb  = TensorBoardLogger(save_dir=str(paths.tb_train_dir), name=\"tb\")\n",
        "    eval_tb   = TensorBoardLogger(save_dir=str(paths.tb_eval_dir),  name=\"tb\")\n",
        "\n",
        "    # Текстовые логи\n",
        "    train_txt = _safe_logger(f\"train_txt_{paths.run_dir.name}\", paths.txt_train_dir / \"train.log\")\n",
        "    eval_txt  = _safe_logger(f\"eval_txt_{paths.run_dir.name}\",  paths.txt_eval_dir / \"eval.log\")\n",
        "\n",
        "    return LoggerHandles(\n",
        "        train_csv=train_csv, eval_csv=eval_csv,\n",
        "        train_tb=train_tb,   eval_tb=eval_tb,\n",
        "        train_txt_logger=train_txt, eval_txt_logger=eval_txt\n",
        "    )\n",
        "\n",
        "def _dump_meta_yaml(paths: RunPaths, run_cfg: RunConfig, train_cfg: TrainConfig, extra: Optional[Dict[str, Any]] = None):\n",
        "    meta = {\n",
        "        \"run_name\": paths.run_dir.name,\n",
        "        \"algo\": run_cfg.algo_name,\n",
        "        \"env\": run_cfg.env_name,\n",
        "        \"started_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"train_config\": asdict(train_cfg),\n",
        "    }\n",
        "    if extra:\n",
        "        meta[\"extra\"] = extra\n",
        "    with open(paths.meta_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(meta, f, allow_unicode=True, sort_keys=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39743834",
      "metadata": {},
      "source": [
        "# Progress Bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "758e6025",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def pbar_create(total_frames: int) -> tqdm:\n",
        "    return tqdm(total=total_frames, desc=\"initializing...\", leave=True, dynamic_ncols=True)\n",
        "\n",
        "def pbar_update(pbar: tqdm, batch_frames: int, desc_parts: Dict[str, Any]):\n",
        "    # аккуратное обновление, чтобы не переполнить total\n",
        "    inc = min(batch_frames, pbar.total - pbar.n)\n",
        "    if inc > 0:\n",
        "        pbar.update(inc)\n",
        "    # человекочитаемое описание\n",
        "    formatted = []\n",
        "    for k, v in desc_parts.items():\n",
        "        if isinstance(v, float):\n",
        "            formatted.append(f\"{k}={v: .4f}\")\n",
        "        else:\n",
        "            formatted.append(f\"{k}: {v}\")\n",
        "    pbar.set_description(\", \".join(formatted))\n",
        "\n",
        "def pbar_write(pbar: tqdm, text: str):\n",
        "    pbar.write(text)\n",
        "\n",
        "def pbar_close(pbar: tqdm):\n",
        "    pbar.close()\n",
        "\n",
        "# ---- Логирование ----\n",
        "\n",
        "def _current_lr(optim: torch.optim.Optimizer, scheduler=None) -> float:\n",
        "    if scheduler is not None:\n",
        "        try:\n",
        "            return float(scheduler.get_last_lr()[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # берем lr первой группы\n",
        "    return float(optim.param_groups[0][\"lr\"])\n",
        "\n",
        "def log_train_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    # CSV/TensorBoard\n",
        "    logs.train_csv.log_metrics(metrics, step=step)\n",
        "    logs.train_tb.log_metrics(metrics, step=step)\n",
        "    # TXT\n",
        "    logs.train_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n",
        "\n",
        "def log_eval_metrics(\n",
        "    logs: LoggerHandles, \n",
        "    metrics: Dict[str, float], \n",
        "    step: int\n",
        "):\n",
        "    logs.eval_csv.log_metrics(metrics, step=step)\n",
        "    logs.eval_tb.log_metrics(metrics, step=step)\n",
        "    logs.eval_txt_logger.info(\" | \".join([f\"{k}={v}\" for k, v in metrics.items()]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b67a6e",
      "metadata": {},
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b3e46dee",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(\n",
        "    paths: RunPaths,\n",
        "    *,\n",
        "    actor: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[Any],\n",
        "    step: int,\n",
        "    eval_metrics: Dict[str, Any],\n",
        "    algo_name: str,\n",
        "    env_name: str,\n",
        "    keep_last_k: int = 3\n",
        ") -> Path:\n",
        "    avg_ret = eval_metrics.get(\"return_mean\", None)\n",
        "    tag = f\"step{step}\"\n",
        "    if avg_ret is not None:\n",
        "        # безопасное имя файла\n",
        "        safe_ret = re.sub(r\"[^0-9eE\\-\\.+]\", \"\", f\"{avg_ret:.3f}\")\n",
        "        tag += f\"_ret{safe_ret}\"\n",
        "    ckpt_path = paths.ckpt_dir / f\"{algo_name}_{env_name}_{tag}.pt\"\n",
        "\n",
        "    payload = {\n",
        "        \"step\": step,\n",
        "        \"algo\": algo_name,\n",
        "        \"env\": env_name,\n",
        "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"model_state_dict\": actor.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"run_dir\": str(paths.run_dir),\n",
        "    }\n",
        "    torch.save(payload, ckpt_path)\n",
        "\n",
        "    # Retention policy: оставить только N последних\n",
        "    ckpts = sorted(paths.ckpt_dir.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if len(ckpts) > keep_last_k:\n",
        "        for old in ckpts[:-keep_last_k]:\n",
        "            try:\n",
        "                old.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return ckpt_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c93040",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2fe88d15",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def setup_run(\n",
        "    root: Path, algo_name: str, env_name: str,\n",
        "    train_cfg: TrainConfig,\n",
        "    run_cfg: Optional[RunConfig] = None,\n",
        "    meta_extra: Optional[Dict[str, Any]] = None\n",
        ") -> Tuple[RunPaths, LoggerHandles, RunConfig]:\n",
        "    run_cfg = run_cfg or RunConfig(root=root, algo_name=algo_name, env_name=env_name)\n",
        "    paths = _make_run_dirs(run_cfg)\n",
        "    _dump_meta_yaml(paths, run_cfg, train_cfg, extra=meta_extra)\n",
        "    loggers = _get_loggers(paths)\n",
        "    return paths, loggers, run_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "875bff8b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10e58c55a2004b70905fd64ed493252a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "initializing...:   0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval: avg reward = 118.453, max episode length = 15\n",
            "eval: avg reward = 153.850, max episode length = 22\n"
          ]
        }
      ],
      "source": [
        "# === training.py (пример использования) ===\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "# Ваши заранее определённые объекты/переменные:\n",
        "# env, actor, collector, advantage_module, replay_buffer, loss_module, optim, scheduler, frames_per_batch, device\n",
        "# а также общее число фреймов для тренировки (например, total_frames)\n",
        "# здесь считаем, что total_frames известен извне (например, как параметр эксперимента)\n",
        "\n",
        "# Корневой каталог и названия алгоритма и среды\n",
        "from pathlib import Path\n",
        "ROOT = Path(\"..\").resolve()\n",
        "ALGO_NAME, ENV_NAME = \"ppo\", \"pendulum\"\n",
        "\n",
        "# Конфиги\n",
        "train_cfg = TrainConfig(num_epochs=10, sub_batch_size=64, max_grad_norm=1.0)\n",
        "# eval каждые 5 батчей — совпадает с исходным кодом\n",
        "run_cfg   = RunConfig(root=ROOT, algo_name=ALGO_NAME, env_name=ENV_NAME, eval_every_batches=5, eval_episodes=5, keep_last_k_ckpts=3)\n",
        "\n",
        "# Инициализация run-директории, логгеров и метаинформации\n",
        "meta_extra = {\n",
        "    \"frames_per_batch\": int(frames_per_batch),\n",
        "    \"device\": str(device),\n",
        "}\n",
        "paths, logs, run_cfg = setup_run(ROOT, ALGO_NAME, ENV_NAME, train_cfg, run_cfg, meta_extra=meta_extra)\n",
        "\n",
        "# Подготовка прогресс-бара\n",
        "# total_frames — количество фреймов, которое вы планируете собрать/обучить за весь ран\n",
        "pbar = pbar_create(total_frames=total_frames)\n",
        "\n",
        "global_frames = 0  # будем логировать шаги в терминах фреймов (удобно для RL)\n",
        "batch_index = 0\n",
        "\n",
        "try:\n",
        "    for i, tensordict_data in enumerate(collector):\n",
        "        batch_index += 1\n",
        "\n",
        "        # === Обучение на партии ===\n",
        "        for _ in range(train_cfg.num_epochs):\n",
        "            # Advantage пересчитываем на каждом проходе\n",
        "            advantage_module(tensordict_data)\n",
        "            data_view = tensordict_data.reshape(-1)\n",
        "            replay_buffer.extend(data_view.cpu())\n",
        "            # Разбиваем на саббатчи\n",
        "            iters = int(frames_per_batch) // int(train_cfg.sub_batch_size)\n",
        "            for _ in range(iters):\n",
        "                subdata = replay_buffer.sample(train_cfg.sub_batch_size)\n",
        "                loss_vals = loss_module(subdata.to(device))\n",
        "                loss_total = (\n",
        "                    loss_vals[\"loss_objective\"] \n",
        "                    + loss_vals[\"loss_critic\"] \n",
        "                    + loss_vals[\"loss_entropy\"]\n",
        "                )\n",
        "\n",
        "                # Оптимизация\n",
        "                loss_total.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), train_cfg.max_grad_norm)\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "\n",
        "        # шаг планировщика\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # === Подсчёт метрик тренировки ===\n",
        "        # корректное число фреймов в текущей пачке\n",
        "        batch_frames = int(tensordict_data.get((\"next\", \"reward\")).numel())\n",
        "        global_frames += batch_frames\n",
        "\n",
        "        # базовые метрики: средняя награда по партии, лоссы, lr\n",
        "        with torch.no_grad():\n",
        "            avg_reward = float(tensordict_data.get((\"next\", \"reward\")).float().mean().cpu().item())\n",
        "        lr_val = _current_lr(optim, scheduler=scheduler)\n",
        "\n",
        "        train_metrics = {\n",
        "            \"reward\": avg_reward,\n",
        "            \"loss_objective\": float(loss_vals[\"loss_objective\"].detach().cpu().item()),\n",
        "            \"loss_critic\": float(loss_vals[\"loss_critic\"].detach().cpu().item()),\n",
        "            \"loss_entropy\": float(loss_vals[\"loss_entropy\"].detach().cpu().item()),\n",
        "            \"loss_total\": float(loss_total.detach().cpu().item()),\n",
        "            \"lr\": lr_val,\n",
        "            \"batch_frames\": float(batch_frames),   # полезно иметь и в csv\n",
        "            \"global_frames\": float(global_frames)  # для графиков\n",
        "        }\n",
        "        log_train_metrics(logs, train_metrics, step=global_frames)\n",
        "\n",
        "        # === Обновление прогресс-бара ===\n",
        "        pbar_update(\n",
        "            pbar,\n",
        "            batch_frames=batch_frames,\n",
        "            desc_parts={\n",
        "                \"avg reward\": avg_reward,\n",
        "                \"frames\": global_frames,\n",
        "                \"lr\": lr_val\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # === Периодическая оценка ===\n",
        "        if (batch_index % run_cfg.eval_every_batches) == 0:\n",
        "            eval_results = eval_policy(env, actor, episodes=run_cfg.eval_episodes, progress=False)\n",
        "            # ожидаем, что eval_policy вернёт хотя бы 'return_mean' и 'max_episode_lengh'\n",
        "            eval_metrics = {\n",
        "                \"return_mean\": float(eval_results.get(\"return_mean\", float(\"nan\"))),\n",
        "                \"max_episode_length\": float(eval_results.get(\"max_episode_lengh\", float(\"nan\"))),\n",
        "                \"global_frames\": float(global_frames)\n",
        "            }\n",
        "            # логируем отдельно в eval-логи\n",
        "            log_eval_metrics(logs, eval_metrics, step=global_frames)\n",
        "\n",
        "            # пишем в прогресс-бар (не ломая его)\n",
        "            pbar_write(\n",
        "                pbar,\n",
        "                f\"eval: avg reward = {eval_metrics['return_mean']:.3f}, \"\n",
        "                f\"max episode length = {eval_metrics['max_episode_length']:.0f}\"\n",
        "            )\n",
        "\n",
        "            # сохраняем чекпоинт по результатам оценки\n",
        "            ckpt_path = save_checkpoint(\n",
        "                paths,\n",
        "                actor=actor,\n",
        "                optimizer=optim,\n",
        "                scheduler=scheduler,\n",
        "                step=global_frames,\n",
        "                eval_metrics=eval_metrics,\n",
        "                algo_name=ALGO_NAME,\n",
        "                env_name=ENV_NAME,\n",
        "                keep_last_k=run_cfg.keep_last_k_ckpts\n",
        "            )\n",
        "            logs.eval_txt_logger.info(f\"checkpoint saved: {ckpt_path.name}\")\n",
        "\n",
        "        # опционально — условие выхода, если собрали достаточно фреймов\n",
        "        if global_frames >= pbar.total:\n",
        "            break\n",
        "\n",
        "finally:\n",
        "    # Гарантированно закрываем бар (важно для корректного вывода в ноутбуках)\n",
        "    pbar_close(pbar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f68cfcc",
      "metadata": {},
      "source": [
        "# Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4d7abc3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using run_dir: C:\\Users\\werna\\Documents\\GitHub\\AgentsLab\\runs\\ppo_pendulum_20250822-192147\n",
            "Train columns: []\n",
            "Eval columns: []\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'step'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# === Примеры графиков ===\u001b[39;00m\n\u001b[32m     53\u001b[39m plt.figure()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m plt.plot(\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df_train.get(\u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m, pd.Series([\u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m)]*\u001b[38;5;28mlen\u001b[39m(df_train))))\n\u001b[32m     55\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mTrain: average reward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mglobal frames (step)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\werna\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\werna\\miniconda3\\envs\\marl\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'step'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === plotting_example.py (запускать в новой ячейке после обучения) ===\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Укажите путь к последнему ран-каналу, либо найдите автоматически:\n",
        "ROOT = Path(\"..\").resolve()\n",
        "runs_dir = ROOT / \"runs\"\n",
        "\n",
        "# Найдём последний ран (по времени модификации папки)\n",
        "run_dirs = sorted([p for p in runs_dir.glob(\"*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "assert len(run_dirs) > 0, \"Не найдено ни одного запуска в runs/\"\n",
        "run_dir = run_dirs[0]\n",
        "print(\"Using run_dir:\", run_dir)\n",
        "\n",
        "def _load_pl_csv(csv_root: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    CSVLogger из Lightning создаёт иерархию: <csv_root>/<name>/version_x/metrics.csv\n",
        "    и может писать в «длинном» формате (name, step, value) либо «широком» (step + метрики).\n",
        "    Эта функция аккуратно приводить к широкому формату: столбцы — метрики, индекс — step.\n",
        "    \"\"\"\n",
        "    # ищем все варианты metrics.csv\n",
        "    metrics_files = list(csv_root.glob(\"**/metrics.csv\"))\n",
        "    if not metrics_files:\n",
        "        return pd.DataFrame()\n",
        "    # берём последний по времени\n",
        "    metrics_path = sorted(metrics_files, key=lambda p: p.stat().st_mtime)[-1]\n",
        "    df = pd.read_csv(metrics_path)\n",
        "\n",
        "    if {\"name\", \"step\", \"value\"}.issubset(df.columns):\n",
        "        # длинный формат -> pivot\n",
        "        wide = df.pivot_table(index=\"step\", columns=\"name\", values=\"value\", aggfunc=\"last\")\n",
        "        wide.sort_index(inplace=True)\n",
        "        wide.reset_index(inplace=True)\n",
        "        return wide\n",
        "    else:\n",
        "        # уже широкий формат\n",
        "        if \"step\" not in df.columns:\n",
        "            # если step отсутствует — добавим монотонный по индексу\n",
        "            df.insert(0, \"step\", range(len(df)))\n",
        "        return df\n",
        "\n",
        "train_csv_root = run_dir / \"csv_logs\" / \"train\"\n",
        "eval_csv_root  = run_dir / \"csv_logs\" / \"eval\"\n",
        "\n",
        "df_train = _load_pl_csv(train_csv_root)\n",
        "df_eval  = _load_pl_csv(eval_csv_root)\n",
        "\n",
        "print(\"Train columns:\", df_train.columns.tolist())\n",
        "print(\"Eval columns:\", df_eval.columns.tolist())\n",
        "\n",
        "# === Примеры графиков ===\n",
        "plt.figure()\n",
        "plt.plot(df_train[\"step\"], df_train.get(\"reward\", pd.Series([float(\"nan\")]*len(df_train))))\n",
        "plt.title(\"Train: average reward\")\n",
        "plt.xlabel(\"global frames (step)\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "if \"loss_total\" in df_train.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_train[\"step\"], df_train[\"loss_total\"])\n",
        "    plt.title(\"Train: total loss\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"loss_total\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if not df_eval.empty and \"return_mean\" in df_eval.columns:\n",
        "    plt.figure()\n",
        "    plt.plot(df_eval[\"step\"], df_eval[\"return_mean\"], marker=\"o\")\n",
        "    plt.title(\"Eval: return_mean\")\n",
        "    plt.xlabel(\"global frames (step)\")\n",
        "    plt.ylabel(\"return_mean\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea88a1de",
      "metadata": {},
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ade9f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91d7dbc95013412e9eda87a6b073c091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "eval:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'return_mean': 179.49685134887696,\n",
              " 'return_sum': 8974.842567443848,\n",
              " 'max_episode_lengh': 33,\n",
              " 'num_episodes': 50}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from agentslab.runners.evals import eval_policy\n",
        "\n",
        "eval_policy(env, actor, episodes=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "marl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
